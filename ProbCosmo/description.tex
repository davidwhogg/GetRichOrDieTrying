% This document is part of the GetRichOrDieTrying project.
% Copyright 2014 David W. Hogg (NYU).

% # to-do
% - references
% - equations
% - broader impacts
% - management plan

\documentclass[12pt]{article}
\usepackage{fancyheadings, graphicx}
\setlength{\headsep}{2ex}
\input{hogg_nsf}
  \renewcommand{\headrulewidth}{0pt}
  \pagestyle{fancy}
  \lhead{\textsf{David W. Hogg / New Probabilistic Methods for Observational Cosmology}}
  \rhead{\textsf{\thepage}}
  \cfoot{}

\newcommand{\dd}{\mathrm{d}}
\newcommand{\data}{D}
\newcommand{\intrinsic}{\epsilon}
\newcommand{\shear}{\gamma}
\newcommand{\psf}{\psi}

\begin{document}\sloppy\sloppypar

\paragraph{Introduction}
This proposal is to improve the way we make cosmological measurements.
It is to make this improvement by building and bringing to the
community practical tools for performing probabilistic inference with
cosmological data sets.

The scale of cosmological surveys---in terms of sky area ($\pi$ to
$4\pi$ steradians), number of sources ($10^6$ to $10^9$), dynamic
range (the faintest sources can be 8~mag fainter than the brightest
sources), and heterogeneity of scientific objectives (luminosity
functions, expansion history, baryon acoustic feature)---makes them
inhospitable playgrounds for principled probabilistic inference.
Probabilistic inference (think:\ likelihood function and possibly also
Bayes) requires (in the cosmological context) generating the data, in
the sense of either being able to \emph{simulate} the data (with all
its warts) or else being able to write down a \emph{probability
  distribution in the space of the data}.
In either case, the problem is very hard:
The rawest data (pixel intensities read out by the CCD) are related to
the quantities of greatest interest only extremely indirectly, through
a model that includes not just fundamental cosmology but also the
formation of galaxies within the dark-matter density field, star
formation, intergalactic medium physics, emission and absorption and
distortions from the Earth's atmosphere, and an exceedingly complex
telescope and camera.
(We are not going to solve \emph{all} of these problems in this proposal!)

That is, it is for \emph{very good reasons} that the standard practice
of observational cosmology has a ``traditional'' feel to it:
In a typical large-scale structure survey, the standard practice is to
image the sky, create a catalog of sources, obtaining a best value for
each source's redshift (by spectroscopy or photometric redshift
estimation), compute a standard point estimate of the auto-correlation
function, and then perform cosmological inferences on that correlation
function.
This is a daisy-chain of lossy data-analysis steps; information is
lost (and noise propagation becomes approximate or impossible) each
time an intermediate point-estimate is made.
Any juncture at which we can remove one of these lossy steps, or
replace it with an information-preserving step, we will improve our
inferences, in terms of bias, and sometimes also variance.

This proposal is motivated by four observations:
\begin{enumerate}
\item
Cosmological surveys are getting bigger, and our precision demand are
getting higher.
The community has defined ``Stage-III'' and ``Stage-IV'' cosmological
projects; Stage-III is underway now, with projects like \boss and \des
and \panstarrs.
Stage-IV projects, like \lsst, \euclid, and \wfirst will be
multi-billion-source surveys that cover large fractions of the sky and
a huge part of the Hubble volume.
These projects are getting close to saturating the bounds set by the
finite volume of the Observable Universe!
We expect them to see the baryon acoustic feature at multiple redshift
slices, and detect a tiny non-Gaussianity in the initial conditions;
that is, we expect our inferences to improve as the square-root of the
sample size or na\"ive information.
Inference methods that worked in Stage-II and are being used in
Stage-III are not necessarily going to deliver completely in Stage-III
and Stage-IV:
In general, as the data get better and more numerous, inferences only
improve if the systematic errors and model errors are smaller than the
root-variance or precision of the measurements!
That is, we may be getting to a place where bigger is not better.
\item
In statistics, there is a general notion of the trade-off between bias
and variance.\footnote{For the purposes of this proposal, I am going to equate what a
statistician calls a ``bias'' with what a cosmologist calls a
``systematic error'' and what a statistician calls ``variance'' with
the inverse of what a cosmologist calls ``precision''.  I will
generally use the statistician's language.}
The current methods of cosmology make hard choices (cutting a catalog,
for example) which are equivalent (under some analysis) with making
very hard assumptions.
These assumptions are informed and informative---they make inferences
more precise---they reduce the variance of our inferences.
But they only make inferences more \emph{correct} inasmuch as they are
correct.
We may be locking in substantial bias on our current path.
\item
In every important scientific question or data set, there are both
\emph{fundamental parameters} we care about (the cosmological
parameters, say) and \emph{nuisance parameters} we don't.
For example, we might care about the spectral index and amplitude of
the cosmological power spectrum, but we might not care about the
specific redshift of any individual galaxy in the density field.
These galaxy redshifts constitute an example of a \emph{very large}
set of nuisance parameters in any large-scale structure or
weak-lensing analysis.
Because both fundamental and nuisance parameters affect the data, both
matter; they both have to be inferred.
That is, information in the data must flow into both; the nuisances
draw information away from the fundamentals.
We will do better on the fundamentals if we don't come to firm
conclusions about the nuisances; it is better if we either never infer
them, or else marginalize them out of our final inferences about the
fundamentals.
The current standard of making a rigid catalog, point-estimating the
galaxy--galaxy or ellipticity auto-correlation function, and only
\emph{then} doing cosmological inference violates this principle.
We are measuring things we don't need to be.
That must be costing us in precision.
\item
Amazing progress has been made---in the PI's group and in the
community---in performing large, principled inferences.
We have performed astrometry and photometry on imaging without making
hard choices about the catalog limits nor requiring object detections
(Lang \etal, 2009; Brewer \etal, 2013).
There are weak and strong gravitational lensing methods that are fully
probabilistic, reconstructing cluster masses while marginalizing out
all source galaxy redshifts (\eg, Applegate \etal, 2014).
There are first attempts to marginalize out the density field in
large-scale structure (Jasche \& Wandelt, 2013; Jasche \& Lavaux, 2014).
We have performed the first inferences about the exoplanet population
that marginalize out the (very large) uncertainties on the exoplanet
radii (Foreman-Mackey \etal, 2014).
Much of the credit for this goes to applied math and computational
stats; we rely on very fast linear algebra (Ambikasaran \etal, 2014), high-end MCMC
samplers (Brewer \etal, 2013; Foreman-Mackey \etal, 2013), and distributed methods
for hierarchical probabilistic inference (Hogg \etal, 2010).
This proposal capitalizes on and leverages the PI's interdisciplinary
collaborations with applied mathematics and statistics.
\end{enumerate}

All this said, what is proposed here is going to be hard:
The dimensionality of the nuisance parameter space tends to grow with
the size of the data set.
The models of large-scale structure are fundamentally cosmological
simulations, or approximations thereto.

For example, imagine that we want to infer the auto-correlation of the
large-scale structure, but with a catalog that is based on imprecise
photometric redshifts, not precise spectroscopic redshifts.
That is, for each galaxy, there is some function $f(z)$ that expresses
either the likelihood function (probability of the data given the
redshift) or the posterior pdf (probability of the redshift given the
data).
How would we use such information in correlation-function inference?
Current practice---when there are rigid redshift estimates rather than
redshift probability functions---is to count pairs of galaxies and
build the correlation function estimate out of histograms of pair
counts from the data and from ``random'' (unclustered; Poisson)
catalogs with a similar selection function.
With the redshift functions $f(z)$ what to do?
You can resample the data from $f(z)$ and make correlation-function
estimates from the resampled data.
This is \emph{completely wrong}.
It constitutes, in effect, re-convolving the data with the
uncertainty; reducing the precision of inferences and substantially
biasing the results.

The right thing to do is draw samples of the density field \emph{and}
the true redshifts, where the weight of such samples in the answer is
in proportion to the likelihood---the probability that the density
field and the true redshifts can generate the positional data.
This requires an ability to (within the data-analysis iteration loop)
draw density fields.
It also requires a jettison of the whole idea of the point estimate
(Landy--Szalay estimator or whatever) of the two-point function; there
is no use of such an estimator within this inference in which the
redshift inferences are being penalized by their consistency with the
density field.
That alone appears intractable at the present day (but see below), and
this is just one of many problems faced by a principled probabilistic
reasoner in cosmology.

This proposal, in some sense, is to develop the \emph{theory} of
experimental cosmology, plus tools to execute some approximation
tractably.

\paragraph{Why this PI?}
...What have we achieved so far?

...In exoplanets

...In weak lensing; \thetractor.

...How are we making these things possible?  Applied-math developments

...Very fast GPs

...Samplers that don't require tuning

...Importance sampling for hierarchical inference

...Aside:  $p(d\given z)$ vs $p(z\given d)$

...Aside:  ABC on cosmological data

...Pedagogical role in the community

\paragraph{\package{Toolset 1}: Density-field inference and marginalization}

The standard estimator for the auto-correlation function of galaxies
is a point estimator that makes two critical assumptions:
It assumes that the galaxy positions are perfectly known, and that the
density field is a Gaussian random field, with only a mean value and
variance structure.
On the latter point---Gaussianity---the assumption came in by way of
\emph{efficiency}; the estimator only saturates the Cram\'er--Rao
bound if the density field is a Gaussian random field.

If all of cosmology is built on these hard (and known wrong)
assumptions, we can only do better by relaxing them.
We can relax them both if we can make practical ways of generating the
galaxy density field---or mass density field---simultaneously
constrained by the observed galaxy positions \emph{and} the
correlation function.
That is, we need to be able to build a simultaneous probabilistic
model of the three-dimensional data field \emph{and} all of the true
redshifts, constrained by the data (the galaxy positions and the weak
redhift information).

There is one regime in which this can be feasible:
Using a Gaussian Process (Rasmussen \& Williams, 2006), for which we currently have
the fastest code in the world (Ambikasaran \etal, 2014), we can draw density fields
constrained by data for any true-redshift realization, in the case
when the density field is indeed Gaussian!
That is, the project proposed here is easier at larger scales (where
the density is closer to Gaussian) and harder at smaller scales (where
fully cosomological simulations become more important).

In addition, where the density field is weakly non-linear, there is a close
relationship between the density field and the Gaussian initial
conditions, so it is possible to draw Gaussian initial ``latent''
fields and then distort them according to second-order perturbation
theory to make realistic, weakly non-Gaussian density fields.
We believe that this project is feasible; there are examples in
the machine-learning literature of full inference in a model of a
field that is a distortion of a Gaussian random field, and then
sampled by points (galaxies, in our case).

One challenge of this approach is that out-of-the-box Gaussian Process
packages do not permit specification of an \emph{arbitrary}
correlation function (covariance function or kernel function) for the
latent field.
Here, one of the objectives is to \emph{determine} the correlation
function, and indeed \emph{marginalize out} the density field (and all
other nuisances).
This requires a method for specifying arbitrary correlation functions.
With Mike O'Neil (NYU Math) and Jon Wilkening (Berkeley Math), we have
developed a fully general expansion for positive-definite covariance
functions that correspond to free functions for the correlation
function.
This expansion---plus an ability to take large matrix determinants
(which we have; Ambikasaran \etal, 2014)---will permit simultaneous inference of the
latent field controlling the density, the correlation function, and
any aspects of galaxy position (say redshifts) that are uncertain.
Since we will sample in all dimensions with MCMC (or, realistically, a
Gibbs or slice-sampling version of MCMC), it will be possible to
deliver posterior samples for the correlation function \emph{fully
  marginalized} over density and galaxy-position realizations.
This full set of capabilities comprise \package{Toolset~1}.

There is some prior work in this area, most notably by Jasche \&
Wandelt (2013) and Jasche \& Lavaux (2014).
Our differences with them are first, our very fast linear algebra, the
fastest in the world, second, our methods for expanding the
correlation function in real space, and third, our commitment to build
and release useful tools.
That said, the success of Jasche and collaborators is one of the
things that has inspired us to propose building these toolsets.

We propose to develop and execute a method of this kind to perform
large-scale structure inference at intermediate regimes (few Mpc and
larger).
We will test this method on the Stage-III \boss\ galaxy data, and show
that we confirm or improve on results obtained by the standard
methods.
This real-data test will both demonstrate the methods functionally,
and make us into not just the builder of the toolset but also the
\emph{customer} for the toolset.
It is our experience that tools are much better built when the
customer is involved.

It is possible that our work on the \boss\ data will create
different answers (because bias has been reduced) or more precise
answers (because the model is more appropriate) than have been found
previously.
Obviously, if this is the case, we will work to understand the
differences and from where they emerge.

In terms of outputs from this proposal, in addition to the code for
\package{Toolset~1}, we will also publish a theory and method-oriented
paper, and also a data and results paper from the \boss\ experiment.
We will release the toolset as documented, MIT-licensed, open-source
code.

\package{Toolset~1} will make fewer assumptions than the standard
methods for cosmological inference.
In particular, it will estimate the correlation function with an
implicit generative model that is more correct than that underlying
the Landy--Szalay estimator.
It ought, therefore, to provide less biased cosmological results.
However, it will not necessarily provide lower variance results (more
precise results).
This is because the method is \emph{also} more flexible.

The density estimation and correlation inference made possible by
\package{Toolset~1} is very general; it could be applied to other
kinds of density fields, such as the two-dimensional projected
mass-density field observed with weak lensing, and the neutral-gas
density field absorbed through quasar absorption lines.
Inasmuch as time permits, such inferences represent enhanced goals
that could be pursued by the project personnel.

\paragraph{\package{Toolset 2}: Using and improving probabilistic redshift information}

For the faintest sources in the Stage-IV projects (and even, say, the
Stage-II \sdss\ imaging), there is no accurate redshift information.
Indeed, there can't be:
We can never take spectroscopic data as faint as we can do imaging,
and photometric redshifts are not just somewhat imprecise, they are
difficult (or perhaps impossible) to calibrate or test where no
spectroscopy is available.
In addition, we want to be able to use in large-scale structure and
cosmology projects sources that did not get spectroscopy for other
reasons, or where spectroscopy is ambiguous or low in signal-to-noise.
In the brave future of Stage-IV projects, we expect this to be the
vast majority of sources of interest.

Facing this reality, many projects---but in particular \lsst---are
planning on creating and delivering redshift probability products,
which describe either the redshift likelihood function for each source
or else the redshift posterior under simple priors.\footnote{It turns
  out that we \emph{always} want the likelihood function, and
  \emph{never} want the posterior pdf for the redshift.  The reasons
  for this are technical; they relate to the marginalizations that are
  required.  No more will be said about this, but the PI has polemics
  available (``Telescopes don't make catalogs'', Hogg \& Lang 2011).}
The question is:  How should we use these probabilistic outputs?
Most proposed (and some actual) uses of these have turned out to be
misguided, in the sense that they are not expected to improve the
biases in resulting inferences.
That is, there are no \emph{inexpensive} uses of probabilistic
redshift information that improve inferences; all the inexpensive
ideas make things \emph{worse}.

For the purposes of this proposal, two general areas where
probabilistic redshift information might be used will be discussed:
the inference of galaxy luminosity functions (and their evolution) and
the inference of the large-scale structure correlation function.
In each case, it is tempting for an investigator, faced with
probabilistic redshift information, to resample the galaxy population,
drawing for each galaxy randomly from its redshift probability
distrubution.
Then the investigator could perform inference for each such draw and
take the distribution over results to be a propagation of the redshift
noise into the results.
This kind of approach has been performed multiple times in the
cosmological literature.

It turns out that all methods of this kind are incorrect:
They treat the probabilistic redshift functions as applying to the
data---as being tools for distorting the data.
They are not!
Distorting the data is never appropriate, and in the cases of interest
here will enworsen any results.
The redshift functions apply to the model (the latent, true redshifts); they are tools
for distorting the \emph{predictions} to make them match the data.
That is, the probabilistic information about redshift must be used in
the data-generating model.
They must be used as part of a data simulation or a likelihood
function (probability in the data space).
The cool bonus thing is that when they are used in this way---as part
of a generative model---not only do they improve inferences, the
inference will \emph{also} return improved redshift estimates.  That
is, the inference gets better \emph{and} the model provides
information back to the redshifts.
This is the magic of hierarchical modeling.

In the case of luminosity-function inference,
the idea is that any parameterized model of the luminosity function
(even a very general model that is, say, a mixture of Gaussians or a
step function with hundreds of steps) is a model of how galaxy
luminosities and redshifts are generated (by the fundamental or
effective model).
The model for how the data are generated from those luminosities
involves the redshift likelihoods; they take true galaxy redshifts and
luminosities (and other spectral properties) and give back a
probability for the data.
The individual-object redshift likelihood functions are tools that
take a model that can generate true redshifts and convolve that model
into one that generates the \emph{data}.
Then, within that model, all the galaxy true redshifts and
luminosities are model parameters (possibly nuisance parameters) to be
inferred along with the luminosity-function parameters.
The redshifts and luminosities can be marginalized out to provide good
luminosity-function estimates,
or the luminosity-function parameters and luminosities can be marginalized
out to provide better redshift estimates.
\package{Toolset~2} will perform this hierarchical inference.

In the literature, some have ``propagated'' the redshift uncertainty
into the luminosity uncertainty, and measured the luminosity function
using less precise but more conservative luminosity uncertainties.
While this approach is more conservative and less biased than ignoring
the redshift uncertainty, it does not deliver the precision of a
hierarchical inference.
That's because it does not use the learning about the luminosity
function to simultaneously improve the redshift estimation and vice
versa.
It doesn't capture all of the information available from our understanding
that the true redshifts are ``generated'' by the model.
We have demonstrated this (in exoplanet contexts) in several projects
now (Hogg \etal, 2010; Foreman-Mackey \etal, 2014).
Again, \package{Toolset~2} will give us and the community these powers.

In the case of correlation-function inference, the story is very much
the same, only harder to execute:
Now the model has correlation-function parameters, which in turn act
like priors on the a latent ``true'' density field, and then the
galaxies are samples of either that field or else a ``true'' galaxy
field from which galaxies are generated.
(This only works for scales $>$Mpc, where galaxy generation can be
seen as a local, independent process.)
Now each galaxy ``true'' redshift can be informed both by the likelihood
function $f(z)$ and also the current inferences about the density field
that are themselves informed by the correlation function.

This whole chain of reasoning comprises a hierarchical inference that
makes use of \package{Toolset~1} for density-field inference, and
also work we have done in exoplanet contexts (Foreman-Mackey \etal, 2014) to infer a
Gaussian process in the face of data points with noisy \emph{locations}.
This requires, for efficiency, specialized samplers, one
of which we will release along with everything else in
\package{Toolset~2}.

We will use \package{Toolset~2} to measure the luminous red galaxy
(LRG) luminosity function and its evolution, and also the clustering
of luminous red galaxies on intermediate and large scales, in the
\sdss\ imaging data.
One goal is to reproduce what is known about the LRG luminosity
function, another the correlation function, including the baryon
acoustic feature (Eisenstein \etal, 2005).
For the LRG redshifts, we will use not the spectroscopic redshifts but
instead the photometric redshifts, to make sure that $f(z)$ is
non-trivial for all sources.
Because we will be re-inferring for a known population but with worse
data, we---in some sense---know the truth, or what we must get in the
best possible case.
That makes this \sdss\ analysis a true functional test of
\package{Toolset~2}; this is important, because it is such a radical
departure from the standard methodologies.

In the test of \package{Toolset~2} with \sdss\ we can not just try to
reproduce the cosmological quantities, but also to obtain better
posterior beliefs about the redshifts.
This will be another output of the hierarchical model.
Again, since we know the spectroscopic redshifts, we can test these
posterior inferences in a very strong way.

The deliverables of this part of the proposal will be the
\package{Toolset~2} code, and two papers, one that is theory and
method oriented, and another that reports the results of the testing
and inferences on the \sdss\ imaging data.
Again, we will try to understand in these papers the quantitative
differences we get with the (now famous) results of \sdss.

\paragraph{\package{Toolset 3}: Creating and using probabilistic shape and PSF information}

Cosmic shear, small distortion of the images of distant
galaxies by large-scale structure in the universe,
is a major probe of cosmolgy. Measurement of the cosmological
weak lensing signal provides extremely powerful probe of the
evolution of dark energy, since the shear signal depends on
both the growth of structure, and the distance-redshift relation.
In this document we present tractable probabilistic methods that
make less constraining assumption about the flow of information
from the data to the cosmological parameters, and help us fully exploit
the potential of weak lensing in  constraining cosmology using the
imaging data from wide field surveys.

PSF estimation for weak lensing:
Extracting the cosmic shear from galaxy images is one of the most challenging tasks in astronomical
data analysis. Shear signal induced by lensing on the images of background galaxies
is coherent but much smaller than the typical 
ellipticities of those galaxies. Moreover, the image of lensed galaxies is convolved with 
an unknown spatially varying kernel, the Point spread function, whose typical
anisotropy is greater than the cosmic shear itself.
 
Effectively, we have 
noisy estimates of the PSF at the positions of stars. We need to use these estimates to infer the
underlying two dimenstional pattern of the PSF. The spatial variation at large angular scales is 
generally smooth and can be well-modeled by the high signal-to-noise stars. However, both atmospheric
turbulance and telescope optics, introduce PSF that varys on scales smaller than the distance
distance between the bright stars. 

Inaccurate estimation of the PSF at the positions of galaxies will lead to bias in shear estimates,
which will propagate into the dark energy constraints if not accounted for. In order to
tackle this issue, we propose to infer the propability distribution function over the PSF
at the positions of galaxies, so that we can marginalize over the PSF instead of treating it 
as a rigid object.

This probability distribution function (a) needs to be flexible enough to capture the complicated 
features of the shape of the PSF as well as its spatial variation, (b) needs to propagate uncertainties
from the pixel level to the PSF, and (c) needs to be easy to evaluate and easy to sample from. By 
projecting the images of stars into lower dimensional KL basis, and imposing Gaussian Process priors 
over each direction of the KL basis we satisfy all the mentioned requirements. This model successfully propagates
the uncertainties in the form of a posterior PDF over PSFs; 
that is, it returns probabilistic information about the PSF at the positions of galaxies. At each
position of the astronomical image, this posterior PDF has a simple form of a Gaussian.

We also examine the performance of this method on estimation of the PSF for simulations 
of \lsst. First, we identify the bright stars on the image and then we randomly divid
them to training set and validation sets of equal size. After learning the KL basis and 
hyper parameters of the Gaussian Process model in the training set, we infer the posterior 
PDF of PSF at the position of stars in the validation set. Figure [\ref{1}] shows a star
in the validation set, and three different draws from the posterior PDF of PSF at that position.

probabilistic inference of shear:
Typically, after measuring the shapes
of a large ensemble of galaxies in a way that
the effect of PSF is elliminated, ellipticities
of these galaxies are averaged together in order to
find a point estimator of the shear.
The methods based on ensemble averaging make strong assumption
about the probability distribution of the intrinsic shape parameters,
and are able to deliver unbiased estimate of shear only if we
have access to unbiased measurements of individual galaxy
ellipticites. 

Our aim is to deliver a fully probabilistic approach for estimation of shear.
We want to write down a likelihood function $p(\data_n\given\shear)$, where $\{\data_n\}$
represents the set of galaxy images. In order to do so, we need to marginalize over the
nuisance parameters that we do not care about. This set of nuisance paramters includes 
shape parameters (e.g., ellipticities), non-shape parameters (e.g., flux, centroid, etc.)
of galaxies, and also the PSF

\begin{eqnarray}
p(\data_n\given\shear)
  &=& \int p(\data_n\given\intrinsic_n , \omega_{n}, \psf , \shear)
  \,p(\intrinsic_n, \omega_{n})\,p(\psf)\,\dd\intrinsic_n\,\dd\omega_n\,\dd\psf
  \quad ,
\label{integral}
\end{eqnarray}
where $\{\intrinsic\}$ ($\{\omega_{n}\}$) represents the set of shape
(non-shape) parameters.

Schematically, equation (\ref{integral}) can be shown by the probabilistic
graphical model (PGM) in Figure[\ref{2}]. Therefore, in this model we need
a prior PDF over intrinsic (unlensed, PSF deconvolved) parameters of galaxies.
This can be done by (a) finding a deep subset (containing a large number of high
signal-to-noise galaxies) of the data, (b) finding the maximum-likelihood estimates
of parameters of those galaxies, (c) and fitting an empirical prior to those parameters. 

Importance sampling ........

multi-exposure data:
The imaging data from the upcoming WL surveys will consists of several
short exposures of each individual galaxy. Stacking has been widely used
as a method to combine information from multiple exposures. 
Although stacking is able to deliver high signal-to-noise images, it distroys a
huge amount of information regarding the spatial variation of
the PSF, and it involves image transformation that could lead to correlated noise
in the final product and distortion of the PSF.

An important advantage of our model is that it can be easily extended
to making inference of shear from multi-exposure data. The graphical model 
regarding corresponding to application of our probabilistic inference to
multi-exposure data is shown in Figure[\ref{3}].

\paragraph{Broader impacts}

The PI has been an active author of methods, software, and pedagogical
documents for the astrophysics and cosmology communities (CITE STUFF).

...MCMC and model selection Data Analysis Recipes; aimed at undergraduates and beginning graduate students

...Organization and leadership of Hack Days and AstroHackWeek.

\paragraph{Prior NSF support}

The PI has been supported by a Cyber-Enabled Discovery Type I Grant
(IIS-1124794; Hogg, PI), \textit{A Unified Probabilistic Model of
  Astronomical Imaging}.
This grant has supported work by NYU graduate students Daniel
Foreman-Mackey and Dilip Krishnan, and NYU postdoctoral scholar Ross
Fadely.
Foreman-Mackey is finishing his PhD at NYU this Spring, and Krishnan
is now a postdoctoral scholar in computer science at MIT.
The grant was explicitly inter-disciplinary between computer vision
and astrophysics.

This grant has supported 12 refereed publications.
In the category of \textbf{Intellectual Merit},
research highlights from this grant include the most highly used MCMC
sampler in the astrophysics community (Foreman-Mackey \etal, 2013)
with more than 200 citations in one year, the
first-ever near-infrared spectroscopy of exoplanets (Oppenheimer \etal, 2013;
Fergus \etal, 2014), and a likelihood function for the exoplanet population
in terms of (or conditioned on) the pixel values telemetered down by
the NASA \kepler\ Satellite (Foreman-Mackey \etal, 2014).

In the category of \textbf{Broader Impacts},
in addition to the publications, this grant has supported the creation of
many open-source code-bases.
The most notable is that of \package{emcee}, mentioned above, which is
one of the top physics-related github repositories in the world.
But in addition to this, the personnel on this grant have opened and
developed in dozens of other repositories, with code under open
licenses (usually MIT).

Also along the broader-impacts line, Foreman-Mackey and the PI have
been operating Hack Days at the American Astronomical Society
meetings, at the dotastronomy meetings, and at AstroData Hack Week,
which will be called AstroHackWeek going forward.
The PI and Foreman-Mackey have been invited to give talks and
participate in summer schools (including the AstroStatistics school at
Penn State and a European equivalent in the Canary Islands) on
pedagogical subjects related to MCMC sampling, Gaussian Processes,
hierarchical inference, and data analysis.
The combination of computer vision and astronomy supported by the
grant is gaining traction in the astronomical community at large, and
astronomical data sets and problems are becoming more visible in the
computer vision community.

The PI has been supported in the past by other NSF grants, including
NSF Astronomy and Astrophysics Research Grant (AST-0908357; Hogg, PI),
\textit{Dynamical models from kinematic data:\ The Milky Way Disk and Halo,}
NSF Information Technology Research Grant (AST-0428465; Hogg, PI),
\textit{Automated Astrometry for Time-Domain and Distributed
Astrophysics,} and
NSF Group Grant (PHY-0101738; Farrar, PI), \textit{Theoretical
Particle Physics, Astrophysics and Cosmology.}
These grants supported postdocs and graduate students and many dozens
of refereed publications.

In addition to this NSF support, the PI also has some NASA support and
is involved in the administration of the Moore--Sloan Data Science
Environment at NYU.
Although the latter grant is a large award, it does not directly
support the research of this PI.
It supports broad data-science programs at NYU which operate
independently of the PI's group or research.

\paragraph{Project management and products}

This project is very lean, it is comprised of just a PI and two
students, one of them part-time.
The rough scheduling would be to work on \package{Toolset~1} and
\package{Toolset~3} in year one, \package{Toolset~2} in year two, and
finish all paper-writing and documentation of code in year three.
The ordering of toolsets is based on the current level of expertise
and comfort with the tools in the PIs group as of today.
It would be nice to write the theory and method papers prior to the
real-data papers, but this is probably not realistic; they will
probably have to be written together, given the interplay between
functional testing and theory in these projects.

The toolsets will be released publicly under the MIT open-source
license using the platform \project{github} or equivalent.
We also expect to work in a generally open mode, so that all of our
work can be seen and inspected as it is underway.
We have had great success in this mode and it has brought a lot of
expertise and visibility to the group and our projects.

The PI will produce, with student help and co-authorship as
appropriate, \project{Data Analysis Recipes} papers, one per year, on
the topics mentioned above.

At two meetings per year, the American Astronomical Society winter
meeting, and the new, annual \project{AstroHackWeek} meeting
co-organized by NYU, University of Washington, and UC Berkeley, we
will operate hack sessions.  These will be organized by the PI, but
with expected participation, both in hacking and in leading and
teaching, by the students.
These hack sessions at these meetings have already been established by
the PI, so these involve no new development or risk; indeed they are
already highly successful operations.

\clearpage

\begin{figure}[!htb]
\minipage{.8\textwidth}
  \includegraphics[width=\linewidth]{112.png}
\endminipage
\caption{Plots showing a star in the validation set
(first column), and three different draws (second column)
from our probabilistic PSF model at the
position of star, and the residuals (third column).\label{1}}
\end{figure}

\begin{figure}[!htb]

\minipage{.3\textwidth}
  \includegraphics[width=\linewidth]{data_e1.png}
\endminipage

\minipage{1.\textwidth}
  \includegraphics[width=\linewidth]{validation_e1.png}
\endminipage

\caption{Top: scatter plot showing the first components of the ellipticities
of the atmospheric PSF in LSST short exposure imaging data. 
Bottom: Map showing the underlying pattern of atmospheric PSF ellipticities (left),
mean of the posterior probability(middle), and the prediction of 5-th
order polynomial regression (right).
\label{2}}
\end{figure}

\begin{figure}[!htb]
\minipage{.8\textwidth}
  \includegraphics[width=\linewidth]{weaklensing1.png}
\endminipage
\caption{PGM showing the flow of information
from cosmological parameters $\Omega$, PSF at different
exposures $\psf_j$, and 
the underlying distribution of shape parameters 
$\alpha$, to the data at each exposure $\data_{nj}$.\label{3}}
\end{figure}

\end{document}
