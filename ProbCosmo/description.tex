\documentclass[12pt]{article}
\usepackage{fancyheadings, graphicx}
\setlength{\headsep}{2ex}
\input{hogg_nsf}
  \renewcommand{\headrulewidth}{0pt}
  \pagestyle{fancy}
  \lhead{\textsf{David W. Hogg / New Probabilistic Methods for Observational Cosmology}}
  \rhead{\textsf{\thepage}}
  \cfoot{}

\newcommand{\dd}{\mathrm{d}}
\newcommand{\data}{D}
\newcommand{\intrinsic}{\epsilon}
\newcommand{\shear}{\gamma}
\newcommand{\psf}{\psi}

\begin{document}\sloppy\sloppypar

\paragraph{Introduction}
This proposal is to improve the way we make cosmological measurements.
It is to make this improvement by building and bringing to the
community practical tools for performing probabilistic inference with
cosmological data sets.

The scale of cosmological surveys---in terms of sky area ($\pi$ to
$4\pi$ steradians), number of sources ($10^6$ to $10^9$), dynamic
range (the faintest sources can be 8~mag fainter than the brightest
sources), and heterogeneity of scientific objectives (luminosity
functions, expansion history, baryon acoustic feature)---makes them
inhospitable playgrounds for principled probabilistic inference.
Probabilistic inference (think:\ likelihood function and possibly also
Bayes) requires (in the cosmological context) generating the data, in
the sense of either being able to \emph{simulate} the data (with all
its warts) or else being able to write down a \emph{probability
  distribution in the space of the data}.
In either case, the problem is very hard:
The rawest data (pixel intensities read out by the CCD) are related to
the quantities of greatest interest only extremely indirectly, through
a model that includes not just fundamental cosmology but also the
formation of galaxies within the dark-matter density field, star
formation, intergalactic medium physics, emission and absorption and
distortions from the Earth's atmosphere, and an exceedingly complex
telescope and camera.
(We are not going to solve \emph{all} of these problems in this proposal!)

That is, it is for \emph{very good reasons} that the standard practice
of observational cosmology has a ``traditional'' feel to it:
In a typical large-scale structure survey, the standard practice is to
image the sky, create a catalog of sources, obtaining a best value for
each source's redshift (by spectroscopy or photometric redshift
estimation), compute a standard point estimate of the auto-correlation
function, and then perform cosmological inferences on that correlation
function.
This is a daisy-chain of lossy data-analysis steps; information is
lost (and noise propagation becomes approximate or impossible) each
time an intermediate point-estimate is made.
Any juncture at which we can remove one of these lossy steps, or
replace it with an information-preserving step, we will improve our
inferences, in terms of bias, and sometimes also variance.

This proposal is motivated by four observations:
\begin{enumerate}
\item
Cosmological surveys are getting bigger, and our precision demand are
getting higher.
The community has defined ``Stage-III'' and ``Stage-IV'' cosmological
projects; Stage-III is underway now, with projects like \boss and \des
and \panstarrs.
Stage-IV projects, like \lsst, \euclid, and \wfirst will be
multi-billion-source surveys that cover large fractions of the sky and
a huge part of the Hubble volume.
These projects are getting close to saturating the bounds set by the
finite volume of the Observable Universe!
We expect them to see the baryon acoustic feature at multiple redshift
slices, and detect a tiny non-Gaussianity in the initial conditions;
that is, we expect our inferences to improve as the square-root of the
sample size or na\"ive information.
Inference methods that worked in Stage-II and are being used in
Stage-III are not necessarily going to deliver completely in Stage-III
and Stage-IV:
In general, as the data get better and more numerous, inferences only
improve if the systematic errors and model errors are smaller than the
root-variance or precision of the measurements!
That is, we may be getting to a place where bigger is not better.
\item
In statistics, there is a general notion of the trade-off between bias
and variance.\footnote{For the purposes of this proposal, I am going to equate what a
statistician calls a ``bias'' with what a cosmologist calls a
``systematic error'' and what a statistician calls ``variance'' with
the inverse of what a cosmologist calls ``precision''.  I will
generally use the statistician's language.}
The current methods of cosmology make hard choices (cutting a catalog,
for example) which are equivalent (under some analysis) with making
very hard assumptions.
These assumptions are informed and informative---they make inferences
more precise---they reduce the variance of our inferences.
But they only make inferences more \emph{correct} inasmuch as they are
correct.
We may be locking in substantial bias on our current path.
\item
In every important scientific question or data set, there are both
\emph{fundamental parameters} we care about (the cosmological
parameters, say) and \emph{nuisance parameters} we don't.
For example, we might care about the spectral index and amplitude of
the cosmological power spectrum, but we might not care about the
specific redshift of any individual galaxy in the density field.
These galaxy redshifts constitute an example of a \emph{very large}
set of nuisance parameters in any large-scale structure or
weak-lensing analysis.
Because both fundamental and nuisance parameters affect the data, both
matter; they both have to be inferred.
That is, information in the data must flow into both; the nuisances
draw information away from the fundamentals.
We will do better on the fundamentals if we don't come to firm
conclusions about the nuisances; it is better if we either never infer
them, or else marginalize them out of our final inferences about the
fundamentals.
The current standard of making a rigid catalog, point-estimating the
galaxy--galaxy or ellipticity auto-correlation function, and only
\emph{then} doing cosmological inference violates this principle.
We are measuring things we don't need to be.
That must be costing us in precision.
\item
Amazing progress has been made---in the PI's group and in the
community---in performing large, principled inferences.
We have performed astrometry and photometry on imaging without making
hard choices about the catalog limits nor requiring object detections
(CITE LANG; CITE BREWER).
There are weak and strong gravitational lensing methods that are fully
probabilistic, reconstructing cluster masses while marginalizing out
all source galaxy redshifts (CITE STUFF).
There are first attempts to marginalize out the density field in
large-scale structure (CITE WANDELT) and the intergalactic medium
(CITE JASCHE).
We have performed the first inferences about the exoplanet population
that marginalize out the (very large) uncertainties on the exoplanet
radii (CITE FOREMAN-MACKEY).
Much of the credit for this goes to applied math and computational
stats; we rely on very fast linear algebra (CITE SIVA), high-end MCMC
samplers (CITE MURRAY; CITE BREWER; CITE FOREMAN-MACKEY), and distributed methods
for hierarchical probabilistic inference (CITE HOGG).
This proposal capitalizes on and leverages the PI's interdisciplinary
collaborations with applied mathematics and statistics.
\end{enumerate}

All this said, what is proposed here is going to be hard:
The dimensionality of the nuisance parameter space tends to grow with
the size of the data set.
The models of large-scale structure are fundamentally cosmological
simulations, or approximations thereto.

For example, imagine that we want to infer the auto-correlation of the
large-scale structure, but with a catalog that is based on imprecise
photometric redshifts, not precise spectroscopic redshifts.
That is, for each galaxy, there is some function $f(z)$ that expresses
either the likelihood function (probability of the data given the
redshift) or the posterior pdf (probability of the redshift given the
data).
How would we use such information in correlation-function inference?
Current practice---when there are rigid redshift estimates rather than
redshift probability functions---is to count pairs of galaxies and
build the correlation function estimate out of histograms of pair
counts from the data and from ``random'' (unclustered; Poisson)
catalogs with a similar selection function.
With the redshift functions $f(z)$ what to do?
You can resample the data from $f(z)$ and make correlation-function
estimates from the resampled data.
This is \emph{completely wrong}.
It constitutes, in effect, re-convolving the data with the
uncertainty; reducing the precision of inferences and substantially
biasing the results.

The right thing to do is draw samples of the density field \emph{and}
the true redshifts, where the weight of such samples in the answer is
in proportion to the likelihood---the probability that the density
field and the true redshifts can generate the positional data.
This requires an ability to (within the data-analysis iteration loop)
draw density fields.
It also requires a jettison of the whole idea of the point estimate
(Landy--Szalay estimator or whatever) of the two-point function; there
is no use of such an estimator within this inference in which the
redshift inferences are being penalized by their consistency with the
density field.
That alone appears intractable at the present day (but see below), and
this is just one of many problems faced by a principled probabilistic
reasoner in cosmology.

This proposal, in some sense, is to develop the \emph{theory} of
experimental cosmology, plus tools to execute some approximation
tractably.

\paragraph{Why this PI?}
...What have we achieved so far?

...In exoplanets

...In weak lensing; \thetractor.

...How are we making these things possible?  Applied-math developments

...Very fast GPs

...Samplers that don't require tuning

...Importance sampling for hierarchical inference

...Aside:  $p(d\given z)$ vs $p(z\given d)$

...Aside:  ABC on cosmological data

...Pedagogical role in the community

\paragraph{\package{Toolset 1}: Density-field inference and marginalization}

The standard estimator for the auto-correlation function of galaxies
is a point estimator that makes two critical assumptions:
It assumes that the galaxy positions are perfectly known, and that the
density field is a Gaussian random field, with only a mean value and
variance structure.
On the latter point---Gaussianity---the assumption came in by way of
\emph{efficiency}; the estimator only saturates the Cram\'er--Rao
bound if the density field is a Gaussian random field.

If all of cosmology is built on these hard (and known wrong)
assumptions, we can only do better by relaxing them.
We can relax them both if we can make practical ways of generating the
galaxy density field---or mass density field---simultaneously
constrained by the observed galaxy positions \emph{and} the
correlation function.

...theory paper

...expansion for the correlation function and kernel

...toolset and demo on BOSS data

...ab-initio run on some new data sets; could be XXX

\paragraph{\package{Toolset 2}: Using and improving probabilistic redshift information}

...how would the probabilities come in to luminosity function estimates and correlation function estimates?

...what is the difference between likelihood and posterior information?

...re-infer the BAO using SDSS imaging but only photometric redshifts

\paragraph{\package{Toolset 3}: Creating and using probabilistic shape and PSF information}

Cosmic shear, small distortion of the images of distant
galaxies by large-scale structure in the universe,
is a major probe of cosmolgy. Measurement of the cosmological
weak lensing signal provides extremely powerful probe of the
evolution of dark energy, since the shear signal depends on
both the growth of structure, and the distance-redshift relation.
In this document we present tractable probabilistic methods that
make less constraining assumption about the flow of information
from the data to the cosmological parameters, and help us fully exploit
the potential of weak lensing in  constraining cosmology using the
imaging data from wide field surveys.

PSF estimation for weak lensing:
Extracting the cosmic shear from galaxy images is one of the most challenging tasks in astronomical
data analysis. Shear signal induced by lensing on the images of background galaxies
is coherent but much smaller than the typical 
ellipticities of those galaxies. Moreover, the image of lensed galaxies is convolved with 
an unknown spatially varying kernel, the Point spread function, whose typical
anisotropy is greater than the cosmic shear itself.
 
Effectively, we have 
noisy estimates of the PSF at the positions of stars. We need to use these estimates to infer the
underlying two dimenstional pattern of the PSF. The spatial variation at large angular scales is 
generally smooth and can be well-modeled by the high signal-to-noise stars. However, both atmospheric
turbulance and telescope optics, introduce PSF that varys on scales smaller than the distance
distance between the bright stars. 

Inaccurate estimation of the PSF at the positions of galaxies will lead to bias in shear estimates,
which will propagate into the dark energy constraints if not accounted for. In order to
tackle this issue, we propose to infer the propability distribution function over the PSF
at the positions of galaxies, so that we can marginalize over the PSF instead of treating it 
as a rigid object.

This probability distribution function (a) needs to be flexible enough to capture the complicated 
features of the shape of the PSF as well as its spatial variation, (b) needs to propagate uncertainties
from the pixel level to the PSF, and (c) needs to be easy to evaluate and easy to sample from. By 
projecting the images of stars into lower dimensional KL basis, and imposing Gaussian Process priors 
over each direction of the KL basis we satisfy all the mentioned requirements. This model successfully propagates
the uncertainties in the form of a posterior PDF over PSFs; 
that is, it returns probabilistic information about the PSF at the positions of galaxies. At each
position of the astronomical image, this posterior PDF has a simple form of a Gaussian.

We also examine the performance of this method on estimation of the PSF for simulations 
of \lsst. First, we identify the bright stars on the image and then we randomly divid
them to training set and validation sets of equal size. After learning the KL basis and 
hyper parameters of the Gaussian Process model in the training set, we infer the posterior 
PDF of PSF at the position of stars in the validation set. Figure [\ref{1}] shows a star
in the validation set, and three different draws from the posterior PDF of PSF at that position.

probabilistic inference of shear:
Typically, after measuring the shapes
of a large ensemble of galaxies in a way that
the effect of PSF is elliminated, ellipticities
of these galaxies are averaged together in order to
find a point estimator of the shear.
The methods based on ensemble averaging make strong assumption
about the probability distribution of the intrinsic shape parameters,
and are able to deliver unbiased estimate of shear only if we
have access to unbiased measurements of individual galaxy
ellipticites. 

Our aim is to deliver a fully probabilistic approach for estimation of shear.
We want to write down a likelihood function $p(\data_n\given\shear)$, where $\{\data_n\}$
represents the set of galaxy images. In order to do so, we need to marginalize over the
nuisance parameters that we do not care about. This set of nuisance paramters includes 
shape parameters (e.g., ellipticities), non-shape parameters (e.g., flux, centroid, etc.)
of galaxies, and also the PSF

\begin{eqnarray}
p(\data_n\given\shear)
  &=& \int p(\data_n\given\intrinsic_n , \omega_{n}, \psf , \shear)
  \,p(\intrinsic_n, \omega_{n})\,p(\psf)\,\dd\intrinsic_n\,\dd\omega_n\,\dd\psf
  \quad ,
\label{integral}
\end{eqnarray}
where $\{\intrinsic\}$ ($\{\omega_{n}\}$) represents the set of shape
(non-shape) parameters.

Schematically, equation (\ref{integral}) can be shown by the probabilistic
graphical model (PGM) in Figure[\ref{2}]. Therefore, in this model we need
a prior PDF over intrinsic (unlensed, PSF deconvolved) parameters of galaxies.
This can be done by (a) finding a deep subset (containing a large number of high
signal-to-noise galaxies) of the data, (b) finding the maximum-likelihood estimates
of parameters of those galaxies, (c) and fitting an empirical prior to those parameters. 

Importance sampling ........

multi-exposure data:
The imaging data from the upcoming WL surveys will consists of several
short exposures of each individual galaxy. Stacking has been widely used
as a method to combine information from multiple exposures. 
Although stacking is able to deliver high signal-to-noise images, it distroys a
huge amount of information regarding the spatial variation of
the PSF, and it involves image transformation that could lead to correlated noise
in the final product and distortion of the PSF.

An important advantage of our model is that it can be easily extended
to making inference of shear from multi-exposure data. The graphical model 
regarding corresponding to application of our probabilistic inference to
multi-exposure data is shown in Figure[\ref{3}].

\paragraph{Prior NSF support}

...Fergus grant

...In addition to this NSF support, NASA support and Moore--Sloan.

...Moore--Sloan \emph{not} providing direct research support!

\paragraph{Broader impacts}

\paragraph{Project management and products}

...Just a PI and students

...Theory papers in year 1

...Tool papers with small demos in year 2

...New large-scale data analyses in year 3

...Open-source toolkits

...Hack Days and Weeks

\paragraph{References cited}

\clearpage

\begin{figure}[!htb]
\minipage{.8\textwidth}
  \includegraphics[width=\linewidth]{112.png}
\endminipage
\caption{Plots showing a star in the validation set
(first column), and three different draws (second column)
from our probabilistic PSF model at the
position of star, and the residuals (third column).\label{1}}
\end{figure}

\begin{figure}[!htb]

\minipage{.3\textwidth}
  \includegraphics[width=\linewidth]{data_e1.png}
\endminipage

\minipage{1.\textwidth}
  \includegraphics[width=\linewidth]{validation_e1.png}
\endminipage

\caption{Top: scatter plot showing the first components of the ellipticities
of the atmospheric PSF in LSST short exposure imaging data. 
Bottom: Map showing the underlying pattern of atmospheric PSF ellipticities (left),
mean of the posterior probability(middle), and the prediction of 5-th
order polynomial regression (right).
\label{2}}
\end{figure}

\begin{figure}[!htb]
\minipage{.8\textwidth}
  \includegraphics[width=\linewidth]{weaklensing1.png}
\endminipage
\caption{PGM showing the flow of information
from cosmological parameters $\Omega$, PSF at different
exposures $\psf_j$, and 
the underlying distribution of shape parameters 
$\alpha$, to the data at each exposure $\data_{nj}$.\label{3}}
\end{figure}



\end{document}
