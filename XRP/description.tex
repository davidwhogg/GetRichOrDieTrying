% This file is part of the SolveAllOfAstronomy project.
% The GetRichOrDieTrying project is a different thing entirely.
% Copyright 2019 David W. Hogg and Megan Bedell

% # Style notes
% - Third person mainly (but maybe not exclusively -- use first for emphasis!)
% - Use macros for acronyms, project names, and so on!
% - Use \nasasection{} not \section{} so we can tweak typography

% # To-do list
% - Do we need to explain wobble in greater detail?
% - Choose more or different sentences or phrases to \textbf{}

\documentclass[12pt, letterpaper]{article}
\usepackage{fancyhdr, varioref}
\setlength{\headheight}{3ex}
\setlength{\headsep}{2ex}
\input{hogg_nasa}
  \renewcommand{\headrulewidth}{0pt}
  \pagestyle{fancy}
  \lhead{\textcolor{darkgrey}{\textsf{Hogg \& Bedell / \EPRV\ in the presence of intrinsic stellar variability}}}
  \rhead{\textcolor{darkgrey}{\textsf{\thepage}}}
  \cfoot{}
\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

\nasasection*{Introduction}

In many important regimes, the detection and characterization of exoplanets
requires extreme precision radial-velocity (\EPRV)
measurements.
With \EPRV\ programs we can detect planets in the medium-to-long 
orbital-period regime that is under-represented in transit searches.
We can measure the 
masses (and hence compositions) of transiting planets,
serving directly \TESS\ Mission goals (for example).
And we can characterize host stars and systems in exquisite detail.
With the next generation of \EPRV\ 
instruments, including the \NEID\ Spectrograph in the \NNEXPLORE\ Program,
we hope to detect terrestrial planets on Earth-like orbits.

From a fundamental-physics perspective, there is essentially no limit to how
precisely astronomers might measure the radial velocity variations
of the center of mass of a distant star with an \EPRV\ spectrograph.
Of course in real life, there are real limits, set
by (for example) photon noise, stellar surface convection and activity,
variations in the atmosphere, and spectrograph calibration stability.
In practice, almost no stars\footnote{%
  At least not in the literature! We have heard rumors and 
  seen flashes of figures suggesting that the \acronym{ESO} spectrograph
  \ESPRESSO\ \citep{Pepe2010} and the Yale spectrograph \EXPRES\ 
  \citep{Jurgenson2016} are seeing 
  30-ish $\cmps$ around certain very quiet stars. This is very promising but 
  unlikely to hold true for the vast majority of stars, or even for these targets during a
  different phase of their activity cycles.}
have had their center-of-mass
velocities measured consistently with an empirical scatter
smaller than $\approx 100\,\cmps$ 
($1\,\mps$).

At the same time, \EPRV\ is responsible for hundreds of \foreign{ab
initio} exoplanet discoveries and hundreds more \foreign{a
posteriori} characterizations of planets discovered elsewhere.
That is, \EPRV\ is hugely successful.
Because most of the discoveries and characterizations have been made
at or near precision limits---that is, near what is detectable at noise levels
greater than $1\,\mps$ per epoch---\textbf{any improvement in precision directly
translates into increased scientific return on investment},
both from existing archival data and from new data from new programs.

\paragraph{Stellar variability is the ``tall pole'':}
Right now the best spectrographs are obtaining internal calibration
consistencies of a few $\cmps$ \citep{espresso-eprv4, expres-eprv4}.
These internal precisions are determined by repeatability and
stability of calibration references such as arc lamps, etalon
(Fabry--Perot) interferometer signals, gas cells, or laser-frequency
combs.

It is therefore now widely accepted (and convincingly
established to us) that the majority of the radial-velocity variance
in \EPRV\ campaigns is coming from the stars themselves, or what's
often called ``astrophysical noise'' (convection, star spots,
asteroseismic pulsations, plages, faculae, and flares, for example).
This proposal is, therefore, \textbf{to address the dominant sources of
  noise in \EPRV\ experiments and to create working methods to
  mitigate them}.
You can precisely calibrate the variations in the spectrograph.
Here we ask: Can you also precisely calibrate the variations in the star?
We believe that the answer is \emph{yes}.

There is also noise coming from telluric lines in the Earth's atmosphere
(especially unmodeled tiny micro-tellurics).
In a separate but related project \citep{Bedell2019} we are addressing this
source of noise as well.
Our work suggests that tellurics and micro-tellurics are addressable problems
and do not currently dominate the end-to-end noise budget.
But it is certainly also an important area for work and study.

\textbf{Our approach to stellar variability is to build data-driven models of what
we don't understand, but with model structure carefully constructed to
match what little we \emph{do} understand.}
That is, to build flexible models which have their capacity tuned to
capture the variations we expect to see. 
Such models will capitalize on our physics-based expectations to
maximize interpretability, while simultaneously having the flexibility to absorb 
the ``unknown unknown'' effects of stellar variability.
%The proposers have strong cred in this area.

\paragraph{Why this team?}
PI Hogg has built novel data-driven models of the color--magnitude diagram of
stars, improving ESA \Gaia\ parallax measurements (\citealt{Leistedt, Widmark, Anderson}).
These models re-purpose machine-learning tools to build models that
have causal structure useful to astronomers (they know about
luminosity distances and parallaxes, for example).
The Co-I Bedell has developed new techniques to make exceedingly
precise measurements of stellar element abundances without explicit
use of stellar models, by designing perfectly differential stellar
comparisons \citep{Bedell2014, Bedell2018}.
PI Hogg has the current best-in-class methods for determining stellar
parameters and abundances from survey-quality spectroscopy (\citealt{Cannon2, Doppel}).
These methods (called \project{The Cannon} after Annie Jump) are
data-driven but designed to capture the smooth variations (with
respect to parameters) that we expect in stellar spectra.

And most relevant of all, PI Hogg and Co-I Bedell built an open-source
pipeline for \EPRV\ data analysis (\wobble; \citealt{Bedell2019}).
This pipeline has many novel aspects to it, but some of the key ideas
are the following: 
It separates stellar and telluric signals in the data in a purely
data-driven way.
It delivers Fisher-information-saturating \RV\ measurements without any
theory-derived or external stellar template; it gets the stellar model
almost losslessly from the data themselves.
It makes use of \project{TensorFlow} and ideas from convex optimization
for computational tractability and performance.
It permits telluric variability and can be extended to permit stellar
variability (that's this proposal).
And it is extensible, flexible, and open source, so that it can be
tweaked and used by different teams with different harware and goals.

In addition to building this software, the proposing team is known for
convening and running hack-week-style workshops (such as \project{Gaia
  Sprint}, \project{Preparing for TESS}, and \project{Telluric Line
  Hack Week}) in which participants learn to use new data sets, new
data-analysis methods, and new software for their scientific goals.
Since a big part of this proposal is dissemination and community adoption,
this track record is very relevant to the current proposal.
And for the long-term value and sustainability of the open-source methods
and software we develop.

\paragraph{Relevance to \NASA\ \XRP\ objectives:} The 2018 \acronym{NSPIRES}
Amendment 68 appendix E.5 (Second Exoplanets Research) description of this
\XRP\ call lists four general categories for proposals (page
E.5-1).
This proposal falls into two of these categories.
It is designed to improve the capabilities of all spectroscopic
facilities performing radial-velocity measurments relevant to
exoplanet observations.
Thus, it is designed to ``Detect exoplanets and/or confirm exoplanet
candidates'', and it is designed to ``Observationally characterize
exoplanets''.
That is, the proposed activities are precisely aligned with
\XRP\ goals.

This proposal is about making ground-based spectroscopic programs more
precise, more capable, and more productive for exoplanet research
goals.
The \XRP\ call for proposals explicitly includes support for
ground-based observing programs, including those at public, private,
and \NASA-supported facilities.

This project is directly aimed at improving the capabilities of the
\NEID\ Spectrograph, which sees first light this year,
as part of the \NASA--\NSF\ \NNEXPLORE\ program.
By serving the \EPRV\ community generally, this project will
directly and indirectly help the \NASA\ \TESS\ Mission,
which requires good \RV\ follow-up for many of its mission goals.
Thus this proposal is written to develop capabilities that will
directly support and improve the legacy value of existing
\NASA\ missions.
It will also make possible new observing programs and strategies for
community proposers to the \NNEXPLORE\ facilities.

Finally, in the context of the US Decadal Survey and proposals for near-future
\NASA\ missions, the question arises: Should we be doing \EPRV\ from space?
The answer to this question depends critically on the detailed error budget on
\EPRV\ measurements.
If \EPRV\ measurements have uncertainties dominated by instrument
stability, atmosphere, and tellurics, then space is the place to be \citep[e.g.][]{Plavchan2018}.
\textbf{If we cannot calibrate out or mitigate stellar variability
  in \EPRV\ measurements, then taking
  \EPRV\ to space should not be a priority.}
Instead, community resources would be much better spent on supporting 
carefully planned ground-based missions which can obtain far more data 
for much cheaper. 
Thus the mitigation of astrophysical noise sources in \EPRV\ is of critical
importance to \NASA's near-term and middle-term planning and strategy
for its exoplanet missions and programs.
Page E.5-2 of the \acronym{NSPIRES} Amendment 68 says ``Proposals
should demonstrate relevance to NASA by describing the benefit for
NASA missions, with specific past, current, or future missions or
programs identified.''  Consider that demonstrated, here and below.

\paragraph{Relevance to \NASA\ Strategic Plans:}
\begin{itemize}
\item
\textit{\NASA\ Strategic Plan 2018}:
The entire Exoplanet Exploration program (of which \XRP\ is a part) falls under
Strategic Objective 1.1 (Understand the Sun, Earth, Solar System, and Universe)
of Strategic Goal 1 (Expand Human Knowledge Through Scientific Discoveries) of the
\NASA\ Strategic Plan.
Because this project is directly addressing the operation of the
ground-based \NNEXPLORE\ spectrograph operated through novel
partnerships, it also connects (perhaps weakly) to Strategic Objective
4.1 (Engage in Partnership Strategies).
\item
\textit{\NASA\ Strategic Space Technology Investment Plan}:
This project has some limited relevance to \NASA's plans for technology development.
Results related to the advisability of performing \EPRV\ work in space are related to
the prioritization of improving technology readiness for spectrograph components.
\item
\textit{Voyages: Charting the Course for Sustainable Human Space
  Exploration}: Since the strategic plan for \NASA\ human exploration
does not yet include plans for planets outside the Solar System
(nothing past Mars?), we admit that our relevance to this part of the
\NASA\ Strategy is weak! But if it did extend outside the Solar System\ldots
\end{itemize}

\paragraph{This proposal, in a nutshell:}
Stellar variability is the tall pole in \EPRV\ at the present day.
We propose to build customized, data-driven models to capture and mitigate these
stellar astrophysical noise sources.
Our goal is to make \EPRV\ measurements an order of magnitude more precise
and thereby transform the science
that can be done with the current and next generation of spectrographs.

\nasasection{First Stage: Information about \EPRV\ in time-varying stars (information)}

Perhaps the strangest thing about what we are proposing here is that
it is \emph{conceptually impossible}!

Okay, not really, but it is worth remembering that the entire
\EPRV\ program is built on the premise that you can measure
\emph{relative} velocities far more precisely than you can measure
\emph{absolute} velocities, because the relative velocity between two
exposures of the same star is seen as a spectral shift from one
exposure to the next.
This point depends on the assumption that the spectrum does not vary
between exposures!
Or that it doesn't vary \emph{adversarially} between exposures.

\hoggbox{\paragraph{\textsl{Reminder}---Information Theory:}
The fundamental limit on \EPRV\ precision is given by the Cramer-Rao Bound, which is
the Fisher Information in this case.
The Fisher Information is the best possible inverse-variance that can be obtained
on any \RV\ measurement, and it appears in the literature in multiple places,
although not usually written in this language \citep{Butler1996, Bouchy2003}.
Symbolically, in the case of a time-invariant spectrum,
the Fisher Information $\sigma_v^{-2}$ is given by
\begin{equation}\label{eq:fisher}
\sigma_v^{-2} =
\left[\frac{\dd f}{\dd v}\right]^{\mathsf T}\cdot C^{-1}\cdot
\left[\frac{\dd f}{\dd v}\right] \quad ,
\end{equation}
where $\sigma_v$ is the \RV\ uncertainty, the derivatives are the derivative
of the spectral expectation $f$ (seen as a column vector) with respect to \RV\ $v$,
and the inverse variance of the noise in the spectrum is $C^{-1}$ (diagonal in the
simple case that all spectral pixels are independently measured; more complex in
what follows!).

Things get a bit more complicated when there are nuisance parameters, but
not fundamentally different: The derivatives
become rectangular projection operators and the Information becomes a tensor (matrix)
must be inverted to deliver nuisance-marginalized uncertainties.

When we turn on stellar variability, there will be two ways to modify the Fisher
Information.
In one framework, the stellar variability is seen as an additional source of noise
in the space of wavelength and time.
The Information gets modified by having $f$ become not just one spectrum but a
full time series of spectra, and by having $C$ become not just the variance tensor
of the instrumental noise, but also of the ``noise'' associated with the spectral
variability (which will be covariant in both wavelength and time).
The additional noise will increase the eigenvalues of $C$, decrease the eigenvalues
of $C^{-1}$, and decrease the precision (increase $\sigma_v$).

In another framework, the stellar variability is seen as an additional set of
nuisance parameters (for example, the amplitudes of eigenspectral contributions
to the spectrum at each observation).
In this framework, the stellar variability is seen as a set of nuisance parameters
that add dimensions to the Information tensor.
This will also serve to reduce the precision (increase $\sigma_v$).
}

Once the spectrum is permitted to vary between exposures (and we know that
it does), the fundamental assumptions of the \EPRV\ literature break down.
In the First Stage (information) we propose to fix this problem, by
working out the conceptual framework for thinking about
\textbf{information theory in \EPRV\ in the presence of a variable
  spectrum}.
We have identified a few approaches to this problem, and we intend to
work down all paths.
This First Stage will produce papers from each of the paths that proves
to deliver useful results or frameworks.
This is the \emph{theory part} of this proposal.

The question is: How precisely can a stellar RV be measured in the context
of stellar spectral variability?
Of course the answer depends on precisely how the star is varying;
in detail it depends on questions about the variability like the following:
\begin{itemize}
\item
\emph{What is the amplitude of the variability?}
If the amplitude is tiny, it won't strongly affect \RV\ measurements.
\item
\emph{What is the dimensionality of the spectral variability?}
If the spectrum varies in a low-dimensional space spanned by a small set
of eigen-spectra, \EPRV\ measurements can be made in the tangent space
orthogonal to the specral variability.
\item
\emph{What is the time coherence of the variability?}
If the variability is uncorrelated from observation to observation, it
can be mitigated by repeat exposures. If it isn't, more complicated
strategies are required.
\item
\emph{What is the projection of the spectral variability onto the RV derivative?}
The most adversarial kind of spectral variability is that which looks
like the derivative of the spectrum with respect to \RV.
\end{itemize}

\noindent
All of these issues will come up again below in the Second Stage (p-modes)
and Third Stage (stochastics).
But in the First Stage, we want to understand precisely and quantitatively
how these issues (amplitude, dimensionality, time coherence, and projection
onto RV derivative) affect fundamental limits on the precision of
\EPRV\ measurements.
That is, in the First Stage we are trying to understand what is theoretically
possible for \EPRV.

There are four approaches we will take in the First Stage (information)
to answering these theory questions,
and the papers we write in the First Stage depend on our success with each
approach, and how appropriate each approach is, given the magnitudes, spectral
shapes, and time structures of the noise sources we find in the data (and in
the Second and Third Stages). 
These four approaches are the following:
\begin{itemize}
\item
\textbf{Pure noise model:}
If the stellar spectral variability has low amplitude, and either has
no spectral or time structure, or else we are pessimistic about
modeling it in any way, we face the \emph{worst-case scenario}.
In this scenario, all we can do is add the spectral noise to the photon noise
in the noise tensor $C$ in the Fisher Information definition (\ref{eq:fisher}),
in the box \vpageref{eq:fisher},
maintaining the spectral correlations in the noise (which are important),
and re-compute the Information in this bad context.
The Fisher Information will get smaller as the stellar variability
gets worse, and the resulting theoretical \EPRV\ uncertainties will
get larger.

In this approach, the idea would be to estimate (either empirically or
theoretically) the excess variance (in spectrum space) induced by the stellar
spectral variability.
This estimation could be performed with something like PCA, or a more
sophisticated latent-variable model (\eg, that of \citealt{HMF}).
That excess variance would be noise that is covariant in wavelength space,
but which could be added directly to the instrument (photon and calibration
and tellurics) noise tensor $C$ in the Information tensor (\ref{eq:fisher}).
The goal of this approach would be to demonstrate how an investigator
can estimate quantitatively the impact of various kinds of observed or
theorized stellar variability, if the investigator has an estimate of
its variance tensor in the observed space.
\item
\textbf{Spectral filtering:}
If the variability in the spectral domain is low-dimensional, then it is
possible to project the data into a space that is orthogonal to the
variability subspace.
This projection will cost some information, but it will return a time-invariant
spectrum and return us to the utopia of the time-invariant Fisher Information
calculation, just with a new noise model.
This approach is appropriate in a well-defined limit of variability (related
to dimensionality) and is
promising so long as the variability subspace does not contain (or come very
close to containing) the derivative of the spectral expectation with respect
to \RV\ (the derivative in the Fisher Information).

In this approach, the idea would be to estimate the (hopefully
low-rank) subspace (eigenspace) in which the stellar spectrum varies.
Once again, this could be estimated with PCA or more sophisticated
methods.
Then the idea is that at each epoch, the spectrum of the star is
described by a mean spectrum, a \RV, and a set of amplitudes with
which the eigenspectra co-add to fit the individual spectrum taken at
that epoch.
These amplitudes become nuisance parameters which complexity the
Information computation, as described in the box \vpageref{eq:fisher}.
In general the existence of this subspace will reduce the expected
\RV\ precision.
However, the amount that it reduces it will depend most strongly on
the projection of the \RV\ derivative of the mean spectrum onto the
subspace.
This method is called ``filtering'' because this information-theoretic
operation is equivalent to projecting the data onto a subpsace that is
tangent to the spectral-variability subspace.
\item
\textbf{Time-domain process priors:}
Variabilities can be incoherent (most are) or coherent (as p-modes are
over short time scales).
However, in long data sets (years), no source of stellar spectral
variability will look identical to a ``Kepler process'', or the time
dependence of \RV\ induced by mixtures of two-body orbits.
Thus, even in the presence of pretty adversarial stellar spectral
variability, \EPRV\ precision can be reobtained by observing for long
times, and separating the final \RV\ measurements over time into those
components that can and cannot be generated by a Kepler process.
These approches work well in certain limits, and work best when we can
understand the time correlations of the spectral variability well, or
when those time correlations are on time scales far from the orbital
times of interest.

In this approach, the idea would be to put a parameterized, rigid
prior on the exoplanet-induce signals (that is, that they are produced
by an N-body Kepler system), and a stochastic prior (like a Gaussian
Process) on the stellar-variability-induced signals, treating the
\RV\ measurements as being created by the sum of these two
processes.
Or even the spectral variability as being caused by these two
processes (if we want to treat the \RV\ shift as a variability).
This approach would require estimation of the statistical properties
(time coherence or covariance as a function of time lag) for the
\RV\ signals induced by the stellar spectral variability.
These can be estimated with statistics that look like auto-correlation
functions.

The information-theoretic question would then be asked in the space
of the Kepler parameters instead of the \RV\ measurements themselves.
And the Gaussian Process would become part of the nuisance space of
the problem.
This would require a substantial reformulation of the information
theory written in the box \vpageref{eq:fisher}, but it is not
difficult in principle.
It would end up answering a different question, however:
We would not ask ``how well can we measure \RV?''; we would ask
``how well can we learn Kepler parameters?''.
This is the best approach if the stellar variability \emph{does} look
like or contain within its subspace the derivative of the mean spectrum
with respect to \RV.
That is, this is the approach for worst-case scenarios.
In non-worst-case it is still useful, however, because this approach
can be combined with the above approaches to deliver
information-theoretic limits on Kepler parameters in the face of
different kinds of spectral variability.
We also propose to look at that question.
\item
\textbf{Causal modeling:}
Finally, for long-term stellar monitoring campaigns, there is an idea
from causal inference that can help us:
If the stellar intrinsic variability is uncorrelated with the orbits
of any planets (and we expect this except for extremely short-period,
locked planets), then there is no sense in which the spectral
variations should be able to \emph{predict} the \RV\ variations that
are planet-induced.
That is, a regression might be able to separate out the intrinsic
stellar variability.
This approach will only work reliably in projects performing very
long-term stellar monitoring, because it is justified only in a hard
limit.
However, there are many such projects starting now.
(This method is conceptually related to things we have done with
\Kepler\ and \Ktwo\ for planet discovery; \citealt{CPM, DFMK2, CPMdiff}.)

This approach is similar to the previous approach (time-domain processes),
but we determine the impact of the variability on the \RV\ measurements
by considering the extent to which the stellar spectrum variations
associated with the induced \RV\ measurements can be used to predict
the \RV.
The idea is to perform regressions and look at the predictive power, and
turn statistics of that predictive power into a quantitative estimate
of the impact on \EPRV\ precision.
The impact will be related to the covariance of the spectral variation
and the \RV\ variation; that is, it will be possible to estimate it
empirically.
\end{itemize}

\noindent
As we work down these approaches, we will develop results and assemble them
into something like two papers on the theory of \EPRV\ in the presence of
stellar spectral variability. \textbf{These two papers, and the associated open-source
code we generate to perform the justifying experiments, will constitute the
key deliverables of this First Stage (information).}
However, this Stage will also deliver advice and methods to
\EPRV\ investigators.
These methods will permit investigators to determine the impact of
various kinds of empirically observed stellar variability on their
end-to-end expected precision,
and help them to understand whether their end-to-end results are
achieving the \RV\ precisions they ought to expect.
\textbf{That is, we will deliver methods for estimating variability, and methods
for estimating the quantitative impact of those variabilities on \RV\ precision.}

One extremely important point, which we want to make in the papers we write
in the First Stage, is the following:
In the case that the stellar spectrum does not vary, the
cross-correlation function (\CCF) between the observed spectrum and a
(very good) spectral template (model spectrum) is a \emph{sufficient
  statistic} for the \RV\ measurement.
That is, the \CCF\ contains all of the \RV\ information in the data.
This is \emph{no longer true} once the spectrum starts to vary!
That means that approaches for dealing with stellar spectral variability
that are confined to considerations about the shape of the \CCF\ are
doomed to fail---or at least doomed to under-perform.
\textbf{Once the spectrum starts to vary, we have to adopt approaches that model
the data in the full two-dimensional space of photon wavelength and time.}
One of our primary high-level goals is to move the community in the
right direction on this point.

\nasasection{Second Stage: Mitigation of asteroseismic variability (p-modes)}

Asteroseismic p-modes deliver a significant source of contamination in \EPRV\
measurements. 
As the star pulsates, its surface sinusoidally moves towards and away from
the observer, imparting a Doppler shift at the (NUMBER) level.
Traditionally these oscillations are dealt with by ``integrating over'' them
with a 15-minute exposure, but recent works have cast doubt on the efficiency 
of this approach \citep{Medina2018, Chaplin2019}. 
While the exact correct method for dealing with p-modes in \EPRV\ has yet 
to be convincingly established, we can regard this noise source as a 
\textit{best-case} scenario for correlated noise. That is, we believe that 
p-modes can be nearly completely mitigated through spectral and temporal modeling, 
because they have two important properties:
The first is that
\textbf{p-modes are temporally coherent} on reasonable time scales (days to months, depending on
stellar type). Therefore, they can be fit as sinusoids over time intervals 
short relative to the coherence time. Furthermore, the frequencies are known,
or can be known, through monitoring (with the \EPRV\ spectrograph itself 
\citep[][and references therein]{Bouchy2003},
or photometrically with a \TESS-like mission).
The second important property is that
\textbf{p-modes have associated temperature signatures}. Indeed, the star gets
hotter when it contracts and cooler when it expands, adiabatically. Although for
G-type stars these temperature variations are tiny, they ought to be visible in
high-signal-to-noise \EPRV\ spectral observations.

Working with these good properties, we propose to explore four
mitigation strategies for p-modes:
\begin{itemize}
\item
\textbf{Choosing resonant integration times:}
There is hope in the \EPRV\ community that cleverly chosen integration
times can obviate p-mode noise \citep[see, \eg][]{Chaplin2019}.
Current work in this area makes somewhat na\"ive assumptions about the
stability of the p-mode amplitudes.
We will revisit this with real data; the \HARPS\ Archive includes
asteroseismic campaigns that can be empirically coadded into different
exposure times and analyzed for residual p-mode noise.
We expect to find that although some exposure times are better than
others, we need to actively mitigate, not passively avoid, p-mode
noise.
\item
\textbf{Resolving modes and fitting them coherently:}
The alternative strategy to p-mode avoidance is to take short exposures,
short enough that they Nyquist-sample the highest-frequency p-modes of
the star.
Any (say, 30-minute) visit to the star could be split into short (say,
2-minute) exposures.
This incurs some observing and read-noise overheads, but permits
resolution of the p-modes directly.
Then the obtained \RV\ measurements can be fit with a sum of coherent
sinusoids, regularized with the (presumed known) power spectrum of the
star.
That is, the amplitudes permitted for the coherent sinusoids would be
forced by regularization (or priors) to be consistent with the
expected p-mode spectrum.
Then the stellar \RV\ measurement relevant to exoplanet detection and
characterization could be corrected for the state of the p-mode
sinusoids during the exposure set.
\item
\textbf{Fitting the observations incoherently:}
The power spectrum of p-modes can be Fourier Transformed into a kernel
for a Gaussian Process.
This Process captures the time-correlations of the p-mode
radial-velocity variations across exposures taken at different times.
Because the integral of a Gaussian Process is also a Gaussian Process,
this kernel can be used to compute the correlations between exposures
taken for any time intervals at any time lags.
Thus it might be possible to fit any set of exposures with a bespoke
Gaussian Process, perfectly suited to capture the p-modes for each
star, including the coherent and incoherent effects all simultaneously.
There has been some work in other domains along this direction (\citealt{DFMGP}),
but it hasn't been applied to \EPRV\ and it is extremely
promising for p-mode mitigation.
\item
\textbf{Simultaneous temperature monitoring:}
It is likely that \RV\ variations from p-modes can be predicted
to some extent with stellar luminosity variations
(Sharon.~X.~Wang, private communication; also see related work on activity by \citealt{Aigrain})
the \RV\ variations are related to the time derivative of the
luminosity variations.
As we note above, luminosity variations will be associated with
temperature variations, and at high signal-to-noise these may be
visible in \EPRV\ campaigns.
To find them, we might need to beat the observed spectra against an
expected temperature derivative at known p-mode frequencies.
That is, we might need to build a matched filter in the joint
two-dimensional domain of wavelength and time.
If we can detect the temperature variations, they will provide great
prior information about \RV\ variations and create another mitigation
strategy.
\end{itemize}

\noindent
Because we have access to public \HARPS\ asteroseismic campaigns for 
several seismic stars including $\beta$ Hyi, $\alpha$ Cen B, and 18 Sco,
we can perform these experiments on existing, real data.
We don't have to make assumptions about how p-modes work.
And over the course of this project, more data sets may appear
with the right properties at even higher precision from \ESPRESSO\ and \NEID.

If we find that mitigation strategies are best when exposures
are short, we will look seriously at the engineering trade-offs
of extra overheads (like read-out) and extra noise from taking
more exposures. These must be balanced in any optimized program
against benefits of mitigating the p-modes.

\textbf{The deliverables from this Second Stage (p-modes) will be ranked
and analyzed strategies and advice for \EPRV\ programs.} It will
also be methods and associated open-source code for each of the successful
mitigation strategies.
The expectation is that this Stage will produce at least one
refereed paper about p-modes. 

Together, the First and Second Stages set the scene for tackling more
pernicious sources of correlated noise in \EPRV. We have now established 
both simulated and real data sets that can be used as a testing ground for 
mitigation strategies that address less well-understood sources of 
stochastic noise.

\nasasection{Third Stage: Mitigation of myriad other variabilities (stochastics)}

The Third Stage is to consider the incoherent variabilities of stars
that are not well understood theoretically, or which imprint
stochastic time-variable signatures on both the spectrum and the
derived \RV\ measurements.
These include (but are not limited to):
\begin{itemize}
\item
\textbf{star spots:}
Star spots create effective \RV\ perturbations because they reduce the
emission from a part of the rotating stellar surface.
That affects \EPRV\ to zeroth order, but at first order there are other
effects of star spots:
They are localized in velocity space, and they have a different
temperature from the surface of the star.
Therefore they ought to be visible in the two-dimensional space of
spectrum and wavelength as a cooler stellar patch slightly more
localized in velocity than the whole stellar surface.
Our approach on star spots, building on prior work (\citealt{Gully}) will
be to consider lower-temperature spectral components with small
velocity shifts try to fit everything simultaneously.
If the spots are reliably lower in temperature, it should be possible
to detect and mitigate them using physical photosphere models, at
least to first order.
This modeling is possible to implement as an extremely straightforward
extension of the \wobble\ framework.
\item
\textbf{plages, faculae, and other surface features:}
These features are similar to star spots and will imprint in similar
ways.
The main difference between star spots and other kinds of surface
features is that we don't have a good idea of the intrinsic spectral
properties of other kinds of features, which can show up as chemical
anomalies (at least in hot stars; \eg, \citealt{doppler}),
temperature anomalies (\eg\ \citealt{Gully}, \citealt{Milbourne2019}),
or mixtures of the two.
If we want to be flexible about surface features, we want to model
them as we propose for star spots, but permitting their spectral
properties to be determined by the data directly.
This is also possible as an extension of the \wobble\ framework.
Indeed it is very natural because in the current \wobble\ setup, the
star and tellurics are both modeled in a completely data-driven way.
In addition to all this, both spots and surface features should show
time coherence, as they rotate in and out of view on the stellar
surface.
As noted in the description of the First Stage (information), this
time coherence could be critical for identifying these effects and
separating them from the \RV\ signals from exoplanets.
\item
\textbf{flares and magnetic activity:}
The concept of ``stellar activity'' is often combining effects of
magnetic flares and star spots.
From our perspective, star spots appear in the data as rotating
surface patches, whereas flares appear as emission lines and possibly
also effective temperature changes in particular lines, again possibly
also with velocity shifts relative because of rotation or
chromospheric velocities.
Because we don't know exactly what to expect here, we might start by
regressing spectra or spectral residuals (away from \wobble\ fits)
against activity indicators to find full-spectrum indicators of
magnetic activity (work along these lines is also happening in the
Cisewski group at Yale; we know this from private communications).
If we can build a low-dimensional data-driven model (from, say, these
regressions or a latent-variable extension), we could hope to correct
the spectra for flaring, and measure the \RV\ off the corrected
spectrum.
\item
\textbf{surface convection:}
Hot material wells up and cold material sinks on the surface of the star.
On average these motions cancel, but they don't fully cancel spectrally.
Thus there are convective blueshifts in stellar spectra.
But the variance of this process also ought to have spectral
properties, and also some time coherence because the convection
pattern is a smoothly varying, slowly rotating property of the stellar
surface.
Our hope is that even this extremely stochastic process will have
low-dimensional structure in the spectral domain (it's mainly
temperature, after all, and some non-LTE effects) and also have
valuable covariance structure in the time domain.
If our hopes are justified, it will be possible to fit a stochastic
process (like a Gaussian Process) to the time domain, in a low-rank
representation of the spectral domain.
That's speculative, but it is worth finding out if it is true.
\end{itemize}

\noindent
We will not consider all of the possible sources of stochastic variability;
the project staffing and timeline are not large enough.
We will instead prioritize around the intersection of team interests (and
especially GRA interests), opportunity relative to other community efforts
(to use our resources for highest impact), and opportunity in terms of
impact on \EPRV\ projects (amplitudes of signals, flowed down to final measurements).
Since we don't yet understand these variabilities in detail, part of this
proposal is to do the work to make those priority decisions.

As we said in the Introduction, the key idea is that the form of the
data-driven models we generate match the form of the variability that
we expect.
So, for example, when we build a model to capture star spots, we want
to give the model the structure that we expect to have in star spots,
but the flexibility to adapt to the data.
This might look like adding to the \wobble\ model extra spectral
components that have the flexibility to look like cooler star surface
spots, and which have velocities relative to the mean stellar surface
velocity in the range permitted by the stellar rotation ($v\sin i$).
It might additionally involve requiring observations that are close
in time to have these additional spot components have velocities that
are close in velocity space.
After all, we expect star spots to be patches of star with a different
spectrum, moving continuously relative to the stellar mean velocity.
Such additional components could then be optimized along with the other
components of \wobble.
That's just an example, but shows the kinds of approaches we imagine taking.

Again, for this Third Stage (stochastics), we will do experiments on
public \HARPS\ data from the Archive.
That is, all the data necessary for this Stage is in hand.
In addition, we expect more relevant data to appear in public archives
from the \ESPRESSO\ and \NEID\ instruments.
We will adapt to new data as it becomes available.
In addition, for some of these projects could benefit from Solar
spectra, since the Sun is a great example of the kind of star we are
intested in.
By construction, it is Sun-like.
And it can be taken with very good signal-to-noise and time resoluion.
There are both \HARPS-N and \HARPS\ data on the Sun that will become
available during the project duration.
These data will also be used in the Third Stage.

\textbf{The deliverables of the Third Stage (stochastics) will be papers in the
refereed literature about these stochastic sources of stellar spectral
variability.}
They will also be numerical methods, open-source software, documentation,
and tutorials.
Dissemination will be through papers, conference participation, and
\textbf{community workshops} we run in Years 2 and 3 (more detail below).

\nasasection{Timeline and project management}

Although the three parts of this project are presented as ``stages'',
they will not be staged serially.
Rather we will run all three in parallel to make best use of our
personnel and benefit most from the conceptual and data overlaps of the three stages.
All three stages will involve PI Hogg, Co-I Bedell, and the project
graduate student student (\GRA). 

The First Stage (information) will be led by PI Hogg, with experiments (the
toy-model experiments) performed by the \GRA, and writing by the full
team.
It will occupy parts of Years~1 and 2, producing a paper
in each of those two years.
One paper will focus on the information-theoretic approach to the problem,
and the second will focus on the causal-inference approaches.

The Second Stage (p-modes) will be led by Co-I Bedell, with
experiments performed by the \GRA, and writing by the full team.
The scientific analysis will take place in Years~1 and 2, with
writing and dissemination in Years~2 and 3.
This stage will also produce two papers, one focusing on passive
approaches (nulling), and one focusing on active approaches (resolving
and fitting).
The dissemination part will also include two community workshops on
\EPRV\ as a time-domain problem, in which we build on our experience
of working with radial-velocity teams to bring new methods and
software practices to the community.
These workshops will happen in Years~2 and 3.
They are important to the project because some of the approaches we
will be advocating will be ``outside the box'' for some projects.

The Third Stage (stochastics) will have different components led by PI
Hogg, Co-I Bedell, and the \GRA, with a schedule that depends on the
particular interests of the \GRA.
Each of the sources of stochastic variability will have different
signatures in time--spectrum space (as discussed above); we will use
Year~1 to do exploratory work in the model residuals to set priorities
(which projects will be easier to see in the data, which harder).
That prioritization will create a schedule for projects, with data
analyses and experiments performed by the \GRA, and projects supervised
by PI Hogg and Co-I Bedell as it makes sense.
Some of the exploratory experiments will produce publishable results
to write up in Year~2 and more holistic mitigation will require
methods and software that will be built in Years~2 and 3 and published
and disseminated in Year~3.
Again, we will use the community workshops in Years~2 and 3 as part of
the dissemination

PI Hogg will be responsible for overall project management. This is a
tiny project; most of the management will involve decisions about our
priorities given the experiments we perform in Year~1 and the
challenges we discover there.
PI Hogg will also be responsible for academic supervision of, and
career mentoring for, the \GRA.
The two workshops will be chaired by Co-I Bedell, with PI Hogg and
the \GRA on the organizing committees.

\nasasection{Prior and current \NASA\ support}

Over his career, PI Hogg has had 11 \NASA\ grants, and participated in
many more.
More than half of his refereed publications acknowledge \NASA\ support.
The three most recent grants are the following:

\paragraph{\acronym{80NSSC19K0533} (Bean, PI):
Improving the sensitivity of radial velocity spectrographs with data-driven techniques:}
This project (which involves both PI Hogg and Co-I Bedell on a subcontract)
is to
build out our data-driven model (\wobble) of \EPRV\ spectroscopic data
to work with gas-cell-calibrated spectrographs. This project has only
just started; the subcontract to \NYU\ has not yet arrived.

\paragraph{\acronym{NNX16AC70G} (Hogg PI):
Ultra-precise photometry in crowded fields: A self-calibration approach:}
This small project (\Ktwo\ \acronym{GO} program)
was to bring a data-driven model to light-curve calibration
in the \Ktwo\ extension of the \Kepler\ Mission.
It resulted in a paper (\citealt{CPMdiff}), an open-source codebase,
and a set of methods that wer deployed in the \Ktwo\ microlensing campaign (\citealt{K2C9}).
This grant also supported the PhD research and dissertation of \NYU\ graduate
student Dun Wang (now at Kensho Technologies, Cambridge, MA) and work
by PI Hogg, Bernhard Sch\"olkopf (T\"ubingen), Dan Foreman-Mackey
(Flatiron), and others.

\paragraph{\acronym{NNX12AI50G} (Hogg PI):
The Lives and Deaths of Planets and Stars in the Value-Added UV Photon Catalog:}
In this project we built a new calibration of the \GALEX\ Satellite
data, creating new data products, including in very crowded fields,
which the main Mission data had avoided.
In 2012, \NASA\ ``lent'' the \GALEX\ Spacecraft to the California Institute of Technology.
This grant supported the data analysis, calibration, and measurement
changes that were required as the observatory was operated in new
ways by Caltech.
For example, the new observing extended the sky coverage of the \GALEX\ imaging to
cover the entire Galactic plane, greatly increasing its overlap with
the ESA \Gaia\ Mission data.
But at these higher photon rates, new scan strategies were required and therefore
new calibration strategies were also required.
This grant was used to develop new self-calibration schemes for the Spacecraft.
It created calibrated maps of the Galactic Plane.
It also re-calibrated the \GALEX\ photon stream and supported work
searching for time-domain sources in the photon time stream.

The grant was very productive. Over its four-year duration, it
\textbf{directly supported 24 papers in the astrophysics literature}, on a
range of subjects from modeling images (\eg, \citealt{UNWISE})
to measuring stellar photometry (\eg, \citealt{CPM})
to pedagogical contributions on computational data analysis (\eg, \citealt{MCMC})
in addition to the direct work on the \GALEX\ data (\eg, \citealt{Mohammed}).
Some of the final papers are still in preparation, but the grant
supported the PhD research and dissertations of Dun Wang (NYU PhD
mentioned above) and Steven Mohammed (Columbia PhD).
It also supported the research and careers of Dustin Lang (now
Perimeter) and David Schiminovich (Columbia).

\nasasection{Frequently asked questions}

Since this is a proposal, no-one has actually really asked us
questions \emph{frequently} about it.
But here's pretending!

\paragraph{Why do this now?}
There are a few extremely precise spectrographs operating now, 
and many more on the horizon \citep{Wright2017}.
\EPRV\ is entering a golden era, brought on in part by missions like
\TESS, which require solid follow-up, and in part by a hope that
\EPRV\ can detect the true Earth analogs (long-period, low-mass, rocky
planets around G-type stars).
If we don't solve these stellar spectral-variability issues, these
projects will not live up to their greatest promise.
And the three-year time scale of this grant is well matched to the
commissioning of many new instruments.

\paragraph{Isn't this proposal a giant fishing expedition?}
Kind of! It's true that we (and the community in general) are not yet certain 
which aspects of stellar variability are most critical to address, and which 
methods are best-suited to mitigating them. We are not promising to 
``solve'' stellar activity or stellar spectral variability in this proposal.
Much of our work will be 
experimental. But this proposal is more than a series of experiments. 
We firmly believe that overcoming the hurdle of stellar variability and 
discovering Earth analogs will require data and effort from multiple, 
diverse \EPRV\ instruments and groups. To this end, we will establish 
a common language and statistically-motivated theoretical framework 
to guide the broader community. Through the papers produced in this 
program and through ``hack-week''-style events hosted by us, we aim to 
lay the groundwork that \textit{will} solve the problem of stellar variability 
in \EPRV, regardless of the success rate of the granular experiments we do. 

\paragraph{How can these two influence the whole community?}
We might look like outsiders (and especially the PI).
But we are not: We have one of the very few open-source \EPRV\ codes
(\wobble; \citealt{Bedell2019}), and there are many new projects incorporating 
\wobble\ into their pipelines.
Also, we are partners in the forthcoming \HARPS-3 project
\project{Terra Hunting Experiment}, which has a direct engineering
requirement to obtain better than $20\,cmps$ precision
\RV\ measurements for quiet G-type stars.
\project{Terra Hunting} needs our help on stellar variability, and will adopt
the mitigation strategies we discover and build.
We have also operated many very successful workshops and hack weeks
bringing new methods and new data to the astrophysics community.
These include the \project{Gaia Sprints}, \project{Preparing for
  TESS}, and NASA-funded \project{Telluric-line Hack Week}.
These events have brought together diverse communities and spread
the gospel of good methods.
But above all, the whole \EPRV\ community is very thirsty for new
successes in mitigating noise. If we succeed in the Second and Third
Stages of this project, we will have no trouble developing community
interest.

\paragraph{Isn't this already being done?}
It certainly is true that the community is working hard on time
variability in some \EPRV\ contexts.
However, most of the work (with very notable exceptions) has centered
around stellar activity, and most of the work has centered on the
cross-correlation function (\CCF).
It is our view that this is such a tall pole for \EPRV\ that we need
to launch and support multiple groups working from multiple angles.
We will be different from the community in a number of ways, but one
is that we are considering fully data-driven approaches, and in the
full outer-product space of time and wavelength.
We also have a conceptual bent: We will try to change the conversation
about \EPRV\ and then build mitigation strategies that talk to the
new frameworks and languages.
We have unique things to provide.
This does not represent a duplication of effort.

\paragraph{Isn't it all just about getting better instrument stability and calibration?}
No! New spectrographs are consistently getting few-$\cmps$ consistency
in internal calibration.
The problems are external---in the star and in the atmosphere.
Of course the telluric absorption and micro-tellurics are also
probably important, and we are working on those issues too (\citealt{Bedell2019}).
But there is no approach to $10\,\cmps$-level \EPRV\ without a direct
attack on stellar spectral variability.

\paragraph{You can't ever really get rid of stochastic stellar noise, right?}
If something is called ``noise'' we tend to think ``oh well, it's
noisy''. But if noise is structured---in spectral space or temporal
space or both---then it can be modeled continuously.
This proposal argues that all the important noise sources in \EPRV\ coming
from spectral variability will be structured in both time and wavelength.
Therefore there are handles for modeling and mitigation.
We are optimistic that these approaches will work.
If they don't, at least we will know that we have tried very very hard.

\paragraph{How does this connect to \XRP\ program priorities?}
There are three answers to this. The first is that this project
improves the ability of \EPRV\ projects to detect and characterize
exoplanets, which are two of the main goals of the \XRP\ program.
The second is that the results of this project will directly support
and enhance the science being done in the \NASA--\NSF\ \NNEXPLORE\ Program,
which is deploying the \NEID\ Spectrograph this year,
and also support the \TESS\ Mission and community.
The third is that this work is essential to \NASA's medium-term planning
for new missions: If we can't mitigate stellar variability, there is no
point in taking \EPRV\ to space!

\clearpage
\bibliographystyle{apalike}
\raggedright
\bibliography{description.bib}%general,myref,inprep}

\end{document}
