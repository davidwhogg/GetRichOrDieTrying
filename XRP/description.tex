% This file is part of the SolveAllOfAstronomy project.
% The GetRichOrDieTrying project is a different thing entirely.
% Copyright 2019 David W. Hogg and Megan Bedell

% # Style notes
% - Third person, not first person!!
% - Use macros for acronyms, project names, and so on!
% - Use \nasasection{} not \section{} so we can tweak typography

% # To-do list
% - Write zeroth draft!
% - Put in references.

\documentclass[12pt, letterpaper]{article}
\usepackage{fancyhdr, graphicx}
\setlength{\headsep}{2ex}
\input{hogg_nasa}
  \renewcommand{\headrulewidth}{0pt}
  \pagestyle{fancy}
  \lhead{\textsf{Hogg \& Bedell / \EPRV\ in the presence of intrinsic stellar variability}}
  \rhead{\textsf{\thepage}}
  \cfoot{}
\begin{document}\sloppy\sloppypar\raggedbottom\frenchspacing

BEDELL: BE BOLD, EDIT. DON'T COMMENT, EDIT! THANK YOU!
\begin{itemize}
\item
EDIT IN ``HOGG'' IN PLACES WHERE YOU DETECT UGLINESS OR NEED THINGS.
\item
TRY NOT TO RE-FLOW PARAGRAPHS OR LINES SO I CAN RESOLVE CONFLICTS SENSIBLY.
\item
I REALLY NEED HELP WITH THE LITERATURE, OBVIOUSLY.
\item
BOLDFACE THE MOST IMPORTANT POINTS IN THE PROPOSAL, SO A SKIMMER GETS IT ALL. UNBOLDFACE THINGS THAT WERE STUPID TO BOLDFACE.
\item
I APOLOGIZE FOR SHOUTING, BUT I AM WRITING THIS ON A LOUD AIRPLANE.
\end{itemize}

\nasasection{Introduction}

\noindent
Many fundamental measurements for the detection and characterization of 
exoplanetary systems rest crucially on extreme precision radial-velocity (\EPRV)
measurements. Through \EPRV, we can detect planets in the medium-to-long 
orbital period regime that is mostly lacking from the transit approach; we can measure the 
masses of known transiting planets, a crucial piece of the \TESS\ mission's 
fundamental goal; and we can characterize host stars and systems in exquisite 
detail using the Rossiter-McLaughlin effect. In the next generation of \EPRV\ 
instruments, including the \NEID\ spectrograph, we hope to detect terrestrial 
planets on Earth-like orbits.

From a fundamental perspective, there is essentially no limit to how
precisely astronomers might measure the radial velocity variations
of the center of mass of a distant star with an \EPRV spectrograph.
Of course in real life, there are real limits, set
by (for example) photon noise, stellar surface convection and activity,
variations in the atmosphere, and spectrograph calibration stability.
In practice, almost no stars have had their center-of-mass
velocities measured consistently with an empirical scatter
(\acronym{RMS} around the mean) smaller than $\approx~100\,\cmps$ ($1\,\mps$).
\footnote{At least not in the literature! We have heard rumors and seen flashes
of figures suggesting that the \acronym{ESO} spectrograph
\ESPRESSO\ (CITE) and the Yale spectrograph \EXPRES\ (CITE) are seeing 
30-ish $\cmps$ around certain very quiet stars. This is very promising but 
unlikely to hold true for the vast majority of stars, or even for these targets during a
different phase of their activity cycles.}

At the same time, \EPRV\ is responsible for hundreds of \foreign{ab
initio} exoplanet discoveries and hundreds more \foreign{a
posteriori} characterizations of planets discovered elsewhere.
That is, \EPRV\ is hugely successful.
Because most of the discoveries and characterizations have been made
at or near precision limits---that is, near what is detectable at noise levels
greater than $1\,\mps$ per epoch---\textbf{any improvement in precision directly
translates into increased scientific return on investment},
both from existing archival data and from new data from new programs.

\paragraph{Stellar variability is the ``tall pole'':}
Right now the best spectrographs are obtaining internal calibration
consistencies of a few $\cmps$ (BEDELL: REFERENCE?).
These internal precisions are determined by repeatability and
stability of calibration references such as arc lamps, etalon
(Fabry--Perot) interferometer signals, gas cells, or laser-frequency
combs.

It is therefore now widely accepted (and convincingly
established to us) that the majority of the radial-velocity variance
in \EPRV\ campaigns is coming from the stars themselves, or what's
often called ``astrophysical noise'' (convection, star spots,
asteroseismic pulsations, plages, faculae, and flares, for example).
This proposal is, therefore, \textbf{to address the dominant sources of
  noise in \EPRV\ experiments and to create working methods to
  mitigate them}.
You can precisely calibrate the variations in the spectrograph.
Here we ask: Can you also precisely calibrate the variations in the star?
We believe that the answer is ``yes''.

There is also noise coming from telluric lines in the Earth's atmosphere
(especially unmodeled tiny micro-tellurics).
In a separate but related project (HOGG CITE WOBBLE) we are addressing this
source of noise as well.
Our work suggests that tellurics and micro-tellurics are addressable problems
and do not currently dominate the end-to-end noise budget.
But it is certainly also an important area for work and study.

Our approach to stellar variability is to build data-driven models of what
we don't understand, but with model structure carefully constructed to
match what little we \emph{do} understand. (HOGG MAKE SURE THIS THEME CARRIES THROUGH.)
That is, to build flexible models which have their capacity tuned to
capture the variations we expect to see. 
Such models will capitalize on our physics-based expectations to
maximize interpretability, while simultaneously having the flexibility to absorb 
the ``unknown unknown'' effects of stellar variability.
%The proposers have strong cred in this area.

\paragraph{Why this team?}
PI Hogg has built novel data-driven models of the color--magnitude diagram of
stars, improving ESA \Gaia\ parallax measurements (HOGG CITE).
These models re-purpose machine-learning tools to build models that
have causal structure useful to astronomers (they know about
luminosity distances and parallaxes, for example).
The Co-I Bedell has developed new techniques to make exceedingly
precise measurements of stellar element abundances without explicit
use of stellar models, by designing perfectly differential stellar
comparisons (BEDELL CITE).
PI Hogg has the current best-in-class methods for determining stellar
parameters and abundances from survey-quality spectroscopy (HOGG
CITE).
These methods (called \project{The Cannon} after Annie Jump) are
data-driven but designed to capture the smooth variations (with
respect to parameters) that we expect in stellar spectra.

And most relevant of all, PI Hogg and Co-I Bedell built an open-source
pipeline for \EPRV\ data analysis (\wobble, HOGG CITE).
This pipeline has many novel aspects to it, but some of the key ideas
are the following: 
It separates stellar and telluric signals in the data in a purely
data-driven way.
It delivers Fisher-information-saturating \RV\ measurements without any
theory-derived or external stellar template; it gets the stellar model
almost losslessly from the data themselves.
It makes use of \project{TensorFlow} and ideas from convex optimization
for computational tractability and performance.
It permits telluric variability and can be extended to permit stellar
variability (that's this proposal).
And it is extensible, flexible, and open source, so that it can be
tweaked and used by different teams with different harware and goals.

In addition to building this software, the proposing team is known for
convening and running hack-week-style workshops (such as \project{Gaia
  Sprint}, \project{Preparing for TESS}, and \project{Telluric Line
  Hack Week}) in which participants learn to use new data sets, new
data-analysis methods, and new software for their scientific goals.
Since a big part of this proposal is dissemination and community adoption,
this track record is very relevant to the current proposal.
And for the long-term value and sustainability of the open-source methods
and software we develop.

\paragraph{Relevance to \NASA\ \XRP\ objectives:} The 2018 \acronym{NSPIRES}
Amendment 68 appendix E.5 (Second Exoplanets Research) description of this
\XRP\ call lists four general categories for proposals (page
E.5-1).
This proposal falls into two of these categories.
It is designed to improve the capabilities of all spectroscopic
facilities performing radial-velocity measurments relevant to
exoplanet observations.
Thus, it is designed to ``Detect exoplanets and/or confirm exoplanet
candidates'', and it is designed to ``Observationally characterize
exoplanets''.
That is, the proposed activities are precisely aligned with
\XRP\ goals.

This proposal is about making ground-based spectroscopic programs more
precise, more capable, and more productive for exoplanet research
goals.
The \XRP\ call for proposals explicitly includes support for
ground-based observing programs, including those at public, private,
and \NASA-supported facilites.

This project is directly aimed at improving the capabilities of the
\NEID\ Spectrograph, which sees first light this year.
This spectrograph is part of the \NASA\ \NNEXPLORE\ program.
Thus this proposal is written to develop capabilities that will
directly support and improve the legacy value of existing
\NASA\ missions.
It will also make possible new observing programs and strategies for
community proposers to the \NNEXPLORE\ facility.

Finally, in the context of the US Decadal Survey and proposals for near-future
\NASA\ missions, the question arises: Should we be doing \EPRV\ from space?
The answer to this question depends critically on the detailed error budget on
\EPRV\ measurements.
If \EPRV\ measurements have uncertainties dominated by instrument
stability, atmosphere, and tellurics, then space is the place to be.
\textbf{If we cannot calibrate out or mitigate stellar variability
  in \EPRV\ measurements, then taking
  \EPRV\ to space is not a good use of \NASA\ resources}.
(BEDELL CAN THAT BE PHRASED POSITIVELY?)
Thus the mitigation of astrophysical noise sources is of critical
importance to \NASA's near-term and middle-term planning and strategy
for its exoplanet missions and programs.
Page E.5-2 of the \acronym{NSPIRES} Amendment 68 says ``Proposals
should demonstrate relevance to NASA by describing the benefit for
NASA missions, with specific past, current, or future missions or
programs identified.''  Consider that demonstrated, here and below.

\paragraph{Relevance to \NASA\ Strategic Plans:}
\begin{itemize}
\item
\textit{\NASA\ Strategic Plan 2018}:
The entire Exoplanet Exploration program (of which \XRP\ is a part) falls under
Strategic Objective 1.1 (Understand the Sun, Earth, Solar System, and Universe)
of Strategic Goal 1 (Expand Human Knowledge Through Scientific Discoveries) of the
\NASA\ Strategic Plan.
Because this project is directly addressing the operation of the
ground-based \NNEXPLORE\ spectrograph operated through novel
partnerships, it also connects (perhaps weakly) to Strategic Objective
4.1 (Engage in Partnership Strategies).
\item
\textit{\NASA\ Strategic Space Technology Investment Plan}:
This project has some limited relevance to \NASA's plans for technology development.
Results related to the advisability of performing \EPRV\ work in space are related to
the prioritization of improving technology readiness for spectrograph components.
\item
\textit{Voyages: Charting the Course for Sustainable Human Space
  Exploration}: Since the strategic plan for \NASA\ human exploration
does not yet include plans for planets outside the Solar System
(nothing past Mars?), we admit that our relevance to this part of the
\NASA\ Strategy is weak!
\end{itemize}

\paragraph{This proposal, in a nutshell:}
Stellar variability is the tall pole in \EPRV\ at the present day.
We propose to build customized, data-driven models to capture and mitigate these
stellar astrophysical noise sources.
Our goal is to make \EPRV\ measurements an order of mangnitude more precise
(two orders of magnitude more informative) and thereby transform the science
that can be done with the current and next generation of spectrographs.

\nasasection{First Stage: Information about \EPRV\ in time-varying stars (information)}

Perhaps the strangest thing about what we are proposing here is that
it is \emph{conceptually impossible}!

Okay, not really, but it is worth remembering that the entire
\EPRV\ program is built on the premise that you can measure
\emph{relative} velocities far more precisely than you can measure
\emph{absolute} velocities, because the relative velocity between two
exposures of the same star is seen as a spectral shift from one
exposure to the next.
This point depends on the assumption that the spectrum does not vary
between exposures!
Or that it doesn't vary \emph{adversarially} between exposures.

Once the spectrum is permitted to vary between exposures (and we know that
it does), the fundamental assumptions of the \EPRV\ literature break down.
In the First Stage (information) we propose to fix this problem, by
working out the conceptual framework for thinking about
\textbf{information theory in \EPRV\ in the presence of a variable
  spectrum}.
We have identified a few approaches to this problem, and we intend to
work down all paths.
This First Stage will produce papers from each of the paths that proves
to deliver useful results or frameworks.
This is the \emph{theory part} of this proposal.

The question is: How precisely can a stellar RV be measured in the context
of stellar spectral variability?
Of course the answer depends on precisely how the star is varying;
in detail it depends on questions about the variability like the following:
\begin{itemize}
\item
\emph{What is the amplitude of the variability?}
If the amplitude is tiny, it won't strongly affect \RV\ measurements.
\item
\emph{What is the dimensionality of the spectral variability?}
If the spectrum varies in a low-dimensional space spanned by a small set
of eigen-spectra, \EPRV\ measurements can be made in the tangent space
orthogonal to the specral variability.
\item
\emph{What is the time coherence of the variability?}
If the variability is uncorrelated from observation to observation, it
can be mitigated by repeat exposures. If it isn't, more complicated
strategies are required.
\item
\emph{What is the projection of the spectral variability onto the RV derivative?}
The most adversarial kind of spectral variability is that which looks
like the derivative of the spectrum with respect to \RV.
\end{itemize}
All of these issues will come up again below in the Second Stage (p-modes)
and Third Stage (stochastics).
But in the First Stage, we want to understand precisely and quantitatively
how these issues (amplitude, dimensionality, time coherence, and projection
onto RV derivative) affect fundamental limits on the precision of
\EPRV\ measurements.
That is, in the First Stage we are trying to understand what is theoretically
possible for \EPRV.

BEDELL: SHOULD WE SET THIS PARAGRAPH OUT AS A BOX CALLED ``FISHER INFORMATION''?
As a reminder, in the case in which the stellar spectrum never varies,
the fundamental \EPRV\ precision is limited by the Cramer-Rao Bound, which is
the Fisher Information in this case.
The Fisher Information is the best possible inverse-variance that can be obtained
on any \RV\ measurement, and it appears in the literature in multiple places,
although not usually written in this language (BEDELL CITES?).
Symbolically the Fisher Information $\sigma_v^{-2}$ is given by
\begin{equation}\label{eq:fisher}
\sigma_v^{-2} =
\left[\frac{\dd f}{\dd v}\right]^{\mathsf T}\cdot C^{-1}\cdot
\left[\frac{\dd f}{\dd v}\right] \quad ,
\end{equation}
where $\sigma_v$ is the \RV\ uncertainty, the derivatives are the derivative
of the spectral expectation $f$ (seen as a column vector) with respect to \RV\ $v$,
and the inverse variance of the noise in the spectrum is $C^{-1}$ (diagonal in the
simple case that all spectral pixels are independently measured; more complex in
what follows!).
That is, the inverse of the squared uncertainty (the inverse variance) on the
\RV\ is an inner product of the derivative of the spectrum with
respect to \RV\ with itself, taken through the inverse variance tensor on the
spectral pixels.
Things get a bit more complicated when there are nuisance parameters, but
not fundamentally different (and we know how to deal with it; the derivatives
become rectangular projection operators and the Information tensor (matrix)
must be matrix-inverted to deliver nuisance-marginalized uncertainties).

There are four approaches we will take in the First Stage (information)
to answering these theory questions,
and the papers we write in The First Stage depend on our success with each
approach, and how appropriate each approach is, given the magnitudes, spectral
shapes, and time structures of the noise sources we find in the data (and in
the Second and Third Stages).
These four approaches are the following:
\begin{itemize}
\item
\textbf{Pure noise model:}
If the stellar spectral variability has low amplitude, and either has
no spectral or time structure, or else we are pessimistic about
modeling it in any way, we face the \emph{worst-case scenario}.
In this scenario, all we can do is add the spectral noise to the photon noise
in the noise tensor $C$ in the Fisher Information definition (\ref{eq:fisher})
(maintaining the spectral correlations in the noise, which are important),
and re-compute the Information in this bad context.
The Fisher Information will get smaller as the stellar variability
gets worse, and the resulting theoretical \EPRV\ uncertainties will
get larger.
\item
\textbf{Spectral filtering:}
If the variability in the spectral domain is low-dimensional, then it is
possible to project the data into a space that is orthogonal to the
variability subspace.
This projection will cost some information, but it will return a time-invariant
spectrum and return us to the utopia of the time-invariant Fisher Information
calculation, just with a new noise model.
This approach is appropriate in a well-defined limit of variability (related
to dimensionality) and is
promising so long as the variability subspace does not contain (or come very
close to containing) the derivative of the spectral expectation with respect
to \RV\ (the derivative in the Fisher Information).
\item
\textbf{Time-domain process priors:}
Variabilities can be incoherent (most are) or coherent (as p-modes are
over short time scales).
However, in long data sets (years), no source of stellar spectral
variability will look identical to a ``Kepler process'', or the time
dependence of \RV\ induced by mixtures of two-body orbits.
Thus, even in the presence of pretty adversarial stellar spectral
variability, \EPRV\ precision can be reobtained by observing for long
times, and separating the final \RV\ measurements over time into those
components that can and cannot be generated by a Kepler process.
These approches work well in certain limits, and work best when we can
understand the time correlations of the spectral variability well, or
when those time correlations are on time scales far from the orbital
times of interest.
\item
\textbf{Causal modeling:}
Finally, for long-term stellar monitoring campaigns, there is an idea
from causal inference that can help us:
If the stellar intrinsic variability is uncorrelated with the orbits
of any planets (and we expect this except for extremely short-period,
locked planets), then there is no sense in which the spectral
variations should be able to \emph{predict} the \RV\ variations that
are planet-induced.
That is, a regression might be able to separate out the intrinsic
stellar variability.
This approach will only work reliably in projects performing very
long-term stellar monitoring, because it is justified only in a hard
limit.
However, there are many such projects starting now.
(This method is conceptually related to things we have done with
\Kepler\ and \Ktwo\ for planet discovery (HOGG CITE WANG DFM).)
\end{itemize}

As we work down these approaches, we will develop results and assemble them
into something like two papers on the theory of \EPRV\ in the presence of
stellar spectral variability. These two papers, and the associated open-source
code we generate to perform the justifying experiments, will constitute the
key deliverables of this First Stage (information).

One extremely important point, which we want to make in the papers we write
in the First Stage, is the following:
In the case that the stellar spectrum does not vary, the
cross-correlation function (\CCF) between the observed spectrum and a
(very good) spectral template (model spectrum) is a \emph{sufficient
  statistic} for the \RV\ measurement.
That is, the \CCF\ contains all of the \RV\ information in the data.
This is \emph{no longer true} once the spectrum starts to vary!
That means that approaches for dealing with stellar spectral variability
that are confined to considerations about the shape of the \CCF\ are
doomed to fail---or at least doomed to under-perform.
\textbf{Once the spectrum starts to vary, we have to adopt approaches that model
the data in the full two-dimensional space of photon wavelength and time.}
One of our primary high-level goals is to move the community in the
right direction on this point.
BEDELL: Can you figure out how to cite things here in a way that is constructive,
supportive, and honest?

\nasasection{Second Stage: Mitigation of asteroseismic variability (p-modes)}

Asteroseismic p-modes straightforwardly contaminate any \EPRV\ measurements.
When the star pulsates, its surface sinusoidally moves towards and away from
the observer.
However, p-modes have two very important properties that give us hope that they
can be almost completely mitigated through spectral and temporal modeling:
\begin{itemize}
\item
\textbf{p-modes are temporally coherent} on reasonable time scales (days to months, depending on
stellar type). Therefore, they can be fit as sinusoids over time intervals 
short relative to the coherence time. Furthermore, the frequencies are known,
or can be known, through monitoring (with the \EPRV\ spectrogaph itself (BEDELL CITE),
or photometrically with a \TESS-like mission).
\item
\textbf{p-modes have associated temperature signatures}. Indeed, the star gets
hotter when it contracts and cooler when it expands, adiabatically. Although for
G-type stars these temperature variations are tiny, they ought to be visible in
high-signal-to-noise \EPRV\ spectral observations.
\end{itemize}

Working with these good properties, we propose to explore four
mitigation strategies for p-modes:
\begin{itemize}
\item
\textbf{Choosing resonant integration times:}
There is hope in the \EPRV\ community that cleverly chosen integration
times can obviate p-mode noise (see, \eg, HOGG CITE CHAPMAN).
Current work in this area makes somewhat naive assumptions about the
stability of the p-mode amplitudes.
We will revisit this with real data; the \HARPS\ Archive includes
asteroseismic campaigns that can be empirically coadded into different
exposure times and analyzed for residual p-mode noise.
We expect to find that although some exposure times are better than
others, we need to actively mitigate, not passively avoid, p-mode
noise.
\item
\textbf{Resolving modes and fitting them coherently:}
The alternative strategy to p-mode avoidance is to take short exposures,
short enough that they Nyquist-sample the highest-frequency p-modes of
the star.
Any (say, 30-minute) visit to the star could be split into short (say,
2-minute) exposures.
This incurs some observing and read-noise overheads, but permits
resolution of the p-modes directly.
Then the obtained \RV\ measurements can be fit with a sum of coherent
sinusoids, regularized with the (presumed known) power spectrum of the
star.
That is, the amplitudes permitted for the coherent sinusoids would be
forced by regularization (or priors) to be consistent with the
expected p-mode spectrum.
Then the stellar \RV\ measurement relevant to exoplanet detection and
characterization could be corrected for the state of the p-mode
sinusoids during the exposure set.
\item
\textbf{Fitting the observations incoherently:}
The power spectrum of p-modes can be Fourier Transformed into a kernel
for a Gaussian Process.
This Process captures the time-correlations of the p-mode
radial-velocity variations across exposures taken at different times.
Because the integral of a Gaussian Process is also a Gaussian Process,
this kernel can be used to compute the correlations between exposures
taken for any time intervals at any time lags.
Thus it might be possible to fit any set of exposures with a bespoke
Gaussian Process, perfectly suited to capture the p-modes for each
star, including the coherent and incoherent effects all simultaneously.
There has been some work in other domains along this direction (CITE
SOMETHING), but it hasn't been applied to \EPRV\ and it is extremely
promising for p-mode mitigation.
\item
\textbf{Simultaneous temperature monitoring:}
It has been shown that \RV\ variations from p-modes can be predicted
to some extent with stellar luminosity variations (BEDELL WHAT TO
CITE?); the \RV\ variations are related to the time derivative of the
luminosity variations.
As we note above, luminosity variations will be associated with
temperature variations, and at high signal-to-noise these may be
visible in \EPRV\ campaigns.
To find them, we might need to beat the observed spectra against an
expected temperature derivative at known p-mode frequencies.
That is, we might need to build a matched filter in the joint
two-dimensional domain of wavelength and time.
If we can detect the temperature variations, they will provide great
prior information about \RV\ variations and create another mitigation
strategy.
\end{itemize}

Because we have access to public \HARPS\ campaigns on [BEDELL WHICH STARS?],
we can perform these experiments on existing, real data.
We don't have to make assumptions about how p-modes work.
And over the course of this project, more data sets may appear
with the right properties, from \ESPRESSO\ and \NEID.

If we find that mitigation strategies are best when exposures
are short, we will look seriously at the engineering trade-offs
of extra overheads (like read-out) and extra noise from taking
more exposures. These must be balanced in any optimized program
against benefits of mitigating the p-modes.

The deliverables from this Second Stage (p-modes) will be ranked
and analyzed strategies and advice for \EPRV\ programs. It will
also be methods and associated open-source code for each of the successful
mitigation strategies.
The expectation is that this Stage will produce at least one
refereed paper about p-modes.

\nasasection{Third Stage: Mitigation of myriad other variabilities (stochastics)}

The Third Stage is to consider the incoherent variabilities of stars
that are not well understood theoretically, or which imprint
stochastic time-variable signatures on both the spectrum and the
derived \RV\ measurements.
These include (but are not limited to):
\begin{itemize}
\item
\textbf{star spots:}
Star spots create effective \RV\ perturbations because they reduce the
emission from a part of the rotating stellar surface.
That affects \EPRV\ to zeroth order, but at first order there are other
effects of star spots:
They are localized in velocity space, and they have a different
temperature from the surface of the star.
Therefore they ought to be visible in the two-dimensional space of
spectrum and wavelength as a cooler stellar patch slightly more
localized in velocity than the whole stellar surface.... HOGG...
\item
\textbf{plages, faculi, and other surface features:}
\item
\textbf{flares:}
\item
\textbf{surface convection:}
\end{itemize}
We will not consider all of the possible sources of stochastic variability;
the project staffing and timeline are not large enough.
We will instead prioritize around the intersection of team interests (and
especially GRA interests), opportunity relative to other community efforts
(to use our resources for highest impact), and opportunity in terms of
impact on \EPRV\ (amplitudes of signals, flowed down to final measurements.
Since we don't yet understand these variabilities in detail, part of this
proposal is to do the work to make those priority decisions.

...Again, we will do experiments on \HARPS\ data in hand. Maybe other data if it
starts to appear, like \ESPRESSO\ and \NEID....

...Deliverables here will be methods, papers, and associated open-source code. Also
parameters of data-driven models.

\nasasection{Timeline and project management}

Although the three parts of this project are presented as ``stages'',
they will not be staged serially.
Rather we will run all three in parallel to make best use of our
personnel and benefit most from the conceptual and data overlaps of the three stages.
All three stages will involve PI Hogg, Co-I Bedell, and the project
graduate student student (\GRA). 

The First Stage (information) will be led by PI Hogg, with experiments (the
toy-model experiments) performed by the \GRA, and writing by the full
team.
It will occupy parts of Years~1 and 2, producing a paper
in each of those two years.
One paper will focus on the information-theoretic approach to the problem,
and the second will focus on the causal-inference approaches.

The Second Stage (p-modes) will be led by Co-I Bedell, with
experiments performed by the \GRA, and writing by the full team.
The scientific analysis will take place in Years~1 and 2, with
writing and dissemination in Years~2 and 3.
This stage will also produce two papers, one focusing on passive
approaches (nulling), and one focusing on active approaches (resolving
and fitting).
The dissemination part will also include two community workshops on
\EPRV\ as a time-domain problem, in which we build on our experience
of working with radial-velocity teams to bring new methods and
software practices to the community.
These workshops will happen in Years~2 and 3.
They are important to the project because some of the approaches we
will be advocating will be ``outside the box'' for some projects.

The Third Stage (stochastics) will have different components led by PI
Hogg, Co-I Bedell, and the \GRA, with a schedule that depends on the
particular interests of the \GRA.
Each of the sources of stochastic variability will have different
signatures in time--spectrum space (as discussed above); we will use
Year~1 to do exploratory work in the model residuals to set priorities
(which projects will be easier to see in the data, which harder).
That prioritization will create a schedule for projects, with data
analyses and experiments performed by the \GRA, and projects supervised
by PI Hogg and Co-I Bedell as it makes sense.
Some of the exploratory experiments will produce publishable results
to write up in Year~2 and more holistic mitigation will require
methods and software that will be built in Years~2 and 3 and published
and disseminated in Year~3.
Again, we will use the community workshops in Years~2 and 3 as part of
the dissemination

PI Hogg will be responsible for overall project management. This is a
tiny project; most of the management will involve decisions about our
priorities given the experiments we perform in Year~1 and the
challenges we discover there.
PI Hogg will also be responsible for academic supervision of, and
career mentoring for, the \GRA.
The two workshops will be chaired by Co-I Bedell, with PI Hogg and
the \GRA on the organizing committees.

\nasasection{Prior and current \NASA\ support}

Over his career, PI Hogg has had 11 \NASA\ grants, and participated in
many more.
More than half of his refereed publications acknowledge \NASA\ support.
The three most recent grants are the following:

\paragraph{\acronym{80NSSC19K0533} (Bean, PI):
Improving the sensitivity of radial velocity spectrographs with data-driven techniques:}
This project (which involves both PI Hogg and Co-I Bedell on a subcontract)
is to
build out our data-driven model (\wobble) of \EPRV\ spectroscopic data
to work with gas-cell-calibrated spectrographs. This project has only
just started; the subcontract to \NYU\ has not yet arrived.

\paragraph{\acronym{NNX16AC70G} (Hogg PI):
Ultra-precise photometry in crowded fields: A self-calibration approach:}
This small project (\Ktwo\ \acronym{GO} program)
was to bring a data-driven model to light-curve calibration
in the \Ktwo\ extension of the \Kepler\ Mission.
It resulted in a paper (HOGG CITE), a codebase (HOGG GitHub link),
and a set of methods that were deployed in the \Ktwo\ microlensing campaign (HOGG CITE).
This grant also supported the PhD research and dissertation of \NYU\ graduate
student Dun Wang (now at Kensho Technologies, Cambridge, MA) and work
by PI Hogg, Bernhard Sch\"olkopf (T\"ubingen), Dan Foreman-Mackey
(Flatiron), and others.

\paragraph{\acronym{NNX12AI50G} (Hogg PI):
The Lives and Deaths of Planets and Stars in the Value-Added UV Photon Catalog:}
In this project we built a new calibration of the \GALEX\ Satellite
data, creating new data products, including in very crowded fields,
which the main Mission data had avoided.
In 2012, \NASA\ ``lent'' the \GALEX\ Spacecraft to the California Institute of Technology.
This grant supported the data analysis, calibration, and measurement
changes that were required as the observatory was operated in new
ways by Caltech.
For example, the new observing extended the sky coverage of the \GALEX\ imaging to
cover the entire Galactic plane, greatly increasing its overlap with
the ESA \Gaia\ Mission data.
But at these higher photon rates, new scan strategies were required and therefore
new calibration strategies were also required.
This grant was used to develop new self-calibration schemes for the Spacecraft.
It created calibrated maps of the Galactic Plane.
It also re-calibrated the \GALEX\ photon stream and supported work
searching for time-domain sources in the photon time stream.

The grant was very productive. Over its four-year duration, it
\textbf{directly supported 24 papers in the astrophysics literature}, on a
range of subjects from modeling images (\eg, HOGG CITE)
to measuring stars (\eg, HOGG CITE)
to pedagogical contributions on computational data analysis (\eg, HOGG CITE)
in addition to the direct work on the \GALEX\ data (\eg, HOGG CITE).
Some of the final papers are still in preparation, but the grant
supported the PhD research and dissertations of Dun Wang (NYU PhD
mentioned above) and Steven Mohammed (Columbia PhD).
It also supported the research and careers of Dustin Lang (now
Perimeter) and David Schiminovich (Columbia).

\nasasection{Frequently asked questions}

Since this is a proposal, no-one has actually really asked us that many questions
about it. But here's pretending!

\paragraph{Why do this now?}
There are a few extremely precise spectrographs operating now (BEDELL
CITATIONS), and many more on the horizon (BEDELL CITATIONS?).
\EPRV\ is entering a golden era, brought on in part by missions like
\TESS, which require solid follow-up, and in part by a hope that
\EPRV\ can detect the true Earth analogs (long-period, low-mass, rocky
planets around G-type stars).
If we don't solve these stellar spectral-variability issues, these
projects will not live up to their greatest promise.
And the three-year time scale of this grant is well matched to the
commissioning of many new instruments.

\paragraph{How can these two influence the whole community?}
We might look like outsiders (and especially the PI).
But we are not: We have one of the very few open-source \EPRV\ codes
(\wobble; HOGG CITE WOBBLE), and there are many new projects incorporating 
\wobble\ into their pipelines.
Also, we are partners in the forthcoming \HARPS-3 project
\project{Terra Hunting Experiment}, which has a direct engineering
requirement to obtain better than $20\,cmps$ precision
\RV\ measurements for quiet G-type stars.
\project{Terra Hunting} needs our help on stellar variability, and will adopt
the mitigation strategies we discover and build.
We have also operated many very successful workshops and hack weeks
bringing new methods and new data to the astrophysics community.
These include the \project{Gaia Sprints}, \project{Preparing for
  TESS}, and NASA-funded \project{Telluric-line Hack Week}.
These events have brought together diverse communities and spread
the gospel of good methods.
But above all, the whole \EPRV\ community is very thirsty for new
successes in mitigating noise. If we succeed in the Second and Third
Stages of this project, we will have no trouble developing community
interest.

\paragraph{Hasn't this already been done?}
It certainly is true that the community is working hard on time
variability in some \EPRV\ contexts.
However, most of the work (with very notable exceptions) has centered
around stellar activity, and most of the work has centered on the
cross-correlation function (\CCF).
It is our view that this is such a tall pole for \EPRV\ that we need
to launch and support multiple groups working from multiple angles.
We will be different from the community in a number of ways, but one
is that we are considering fully data-driven approaches, and in the
full outer-product space of time and wavelength.
We also have a conceptual bent: We will try to change the conversation
about \EPRV\ and then build mitigation strategies that talk to the
new frameworks and languages.
We have unique things to provide.
This does not represent a duplication of effort.

\paragraph{Isn't it all just about getting better instrument stability and calibration?}
No! New spectrographs are consistently getting few-$\cmps$ consistency
in internal calibration.
The problems are external---in the star and in the atmosphere.
Of course the telluric absorption and micro-tellurics are also
probably important, and we are working on those issues too (HOGG CITE
WOBBLE).
But there is no approach to $10\,\cmps$-level \EPRV\ without a direct
attack on stellar spectral variability.

\paragraph{You can't ever really get rid of stochastic stellar noise, right?}
If something is called ``noise'' we tend to think ``oh well, it's
noisy''. But if noise is structured---in spectral space or temporal
space or both---then it can be modeled continuously.
This proposal argues that all the important noise sources in \EPRV\ coming
from spectral variability will be structured in both time and wavelength.
Therefore there are handles for modeling and mitigation.
We are optimistic that these approaches will work.
If they don't, at least we will know that we have tried very very hard.

\paragraph{How does this connect to \XRP\ program priorities?}
There are three answers to this. The first is that this project
improves the ability of \EPRV\ projects to detect and characterize
exoplanets, which are two of the main goals of the \XRP\ program.
The second is that the results of this project will directly support
and enhance the science being done in the \NASA\ \NNEXPLORE\ program,
which is deploying the \NEID\ Spectrograph this year.
The third is that this work is essential to \NASA's medium-term planning
for new missions: If we can't mitigate stellar variability, there is no
point in taking \EPRV\ to space.

\clearpage
\nasasection*{References}

(BEDELL CAN YOU START THIS?)

\end{document}
