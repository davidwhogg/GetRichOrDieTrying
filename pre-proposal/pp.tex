\documentclass[11pt, letterpaper]{article}
\usepackage{fancyheadings, color, hyperref}

% hypertex insanity
  \definecolor{linkcolor}{rgb}{0,0,0.4}
  \hypersetup{
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=linkcolor,    % color of internal links
    citecolor=linkcolor,    % color of links to bibliography
    filecolor=linkcolor,    % color of file links
    urlcolor=linkcolor      % color of external links
  }

% other colors
  \definecolor{grey}{rgb}{0.5,0.5,0.5}
  \newcommand{\deemph}[1]{\textcolor{grey}{\footnotesize{#1}}}

% Margins and spaces
  \setlength{\oddsidemargin}{0in}
  \setlength{\topmargin}{0in}
  \setlength{\headsep}{0.20in}
  \setlength{\headheight}{0.25in}
  \setlength{\textheight}{9.00in}
%  \addtolength{\textheight}{-\headsep}
%  \addtolength{\textheight}{-\headheight}
  \addtolength{\topmargin}{-\headsep}
  \addtolength{\topmargin}{-\headheight}
  \setlength{\textwidth}{6.50in}
  \setlength{\parskip}{0.5ex}
% Headings and footing
  \renewcommand{\headrulewidth}{0pt}
  \pagestyle{fancy}
  \lhead{\deemph{David W. Hogg}}
  \chead{\deemph{Extracting Everything from Astronomical Imaging}}
  \rhead{\deemph{\thepage}}
  \cfoot{}

\begin{document}\sloppy\sloppypar

\noindent
Astronomical facilities have taken imaging to cover the sky literally hundreds of times over.
This imaging comes from many different facilities, taken under different conditions,
  taken through different bandpasses and with different kinds of detectors,
  and not (in general) properly archived with correct and standards-compliant meta-data.
With the superpowers obtained for my group through its collaborations with
  applied mathematicians, statisticians, and computer scientists,
  I now believe it is possible to fully exploit the joint information latent in all of these data sets.
In the next few years,
  I plan to use this realization to make possible several ambitious scientific projects,
  in the search for Earth analogs around other stars,
  in cosmological weak lensing,
  and in the cataloging and characterization of the most distant stars and galaxies.

\noindent\textbf{The past:}
I started working on data-driven problems in astrophysics through an unsual connection:
  my friendship with Sam Roweis (now deceased), a machine learning expert and former high-school friend.
When Mike Blanton (now NYU faculty) and I were debating the feasibility of an automated self-calibration
  of astronomical imaging that would have, effectively, several hundred million free parameters,
  Roweis overheard us and pointed out that \emph{if} we could apply a strictly quadratic cost function,
  and \emph{if} we could apply only quadratic regularization terms,
  then the problem would be convex, and therefore ``easy'' to solve.
This realization created two things:
The first is the most precise calibration of the \textsl{Sloan Digital Sky Survey} imaging data,
  and the calibration still in use today.
The second is a realization that I needed to ``get out more'' and talk to colleagues in different fields.

Later, Roweis and I co-advised PhD student Dustin Lang, who took on a seemingly impossible project:
Can you tell, just by looking at the pixel content of an image of the night sky,
  where that image is centered on the sky and what stars it contains?
It is obvious that you can do this for wide-field images:  Humans do it all the time (``there's Orion!''),
  but is it possible for deep, narrow-field, science-grade images of the types taken by serious amateur astronomers
  and professionals?
Through a combination of geometric hashing, generative modeling, and bayesian decision theory---%
  none of which I knew about beforehand---we solved this problem and produced \textsl{Astrometry.net}.
This software package has more than 10,000 downloads, thousands of users (mainly amateurs),
  and has been incorporated into the automated data analysis pipelines of many astronomical projects.

Fundamentally, in my view, the language of data analysis is Bayesian or probabilistic inference.
That said, probabilistic inference scales badly and is hard to do ``at scale''.
For this reason, we have been working with applied mathematicians and statisticians to improve our abilities.
With my students Hou and Foreman-Mackey, and with applied mathematician Jonathan Goodman (NYU) and statistician BrendanBrewer (Auckland),
  we have implemented a set of different Markov-Chain Monte Carlo methods,
  including an affine-invariant ensemble sampler, a bridge sampler, and a nested sampler,
  and we have plans for more.
The key idea is that no sampler is ideal for all problems; it is important to have many options.
We are working on software engineering that makes mixing and switching samplers very easy.
Our current implementation, \textsl{emcee}, is extremely well engineered (by Foreman-Mackey),
  and is averaging more than ten citations per month in the refereed literature.

\noindent\textbf{The future:}
Soon after \textsl{Astrometry.net} was up and running,
  we realized that we had only just scratched the surface:
As all these images pour in and get recognized, they also provide new information about the sky,
  which can be used to improve our models, improve our meaasurements, and revise our catalogs.
Of course, when you open this box, you realize it is a very difficult problem:
  If everything is up for debate and revision, then everything changes from being a catalog entry,
  or a fact about the sky, to being a probability distribution over catalog entries.
This quickly gets intractable.
In one direction on this, we have worked with Brewer,
  who is helping us work towards ``biting the bullet'' and going fully probabilistic;
  with Brewer we wrote a paper in which we fully marginalize over ``catalog space'' for a realistic astronomical image!
In another direction, we have worked with machine-learner Bernhard Sch\"olkopf (MPI-IS),
  who helped us find an approximate solution that can be run ``online'' (streaming) but still benefit
  as much as possible from all new imaging;
  we used this system to find a very low-intensity astronomical feature around NGC 5907,
  not visible in any of the individual images we combined to find it.
In yet another, Lang and I wrote a quirky paper that used amateur observations of Comet Holmes (posted on the web),
  with no other data or meta data,
  to infer the gravitational orbit of the Comet;
  this showed that the combination of arbitrarily different data is possible.

Most of what we want to know about the Universe can be represented by a few numbers:
We want to know the ten-ish cosmological parameters, or, say, the mass and radius of an exoplanet.
The problem is, that we don't get to ``observe'' these numbers directly;
  they are always mixed into our observations by a complex model of auxiliary things that we don't much care about.
For cosmological parameters, they would be easy to measure if we knew how galaxies form,
  but since we don't (yet), we have to instantiate flexible models and, in principle, marginalize them out.
Because this is hard, no-one knows how to do this.
For exoplanets, they would be easy to find if the stars around which they orbit showed no stochastic variability,
  but since they do, we have to instantiate flexible models again.
Even in the relatively straightforward image combination problems above,
  the cameras used to take the imaging have distortions and sensitivity variations
  that we know nothing about and also need to model flexibly if we are going to make accurate inferences.

One motivating fact behind our interest is that the NASA \textsl{Kepler} mission, which has come to an end,
  has not detected any good Earth analog.
It has found habitable-zone planets around Solar-like stars,
  but they are substantially bigger than Earth,
  and it has found habitable-zone rocky planets around much cooler, smaller stars,
  but these stars are very different from the Sun.
A back-of-the-envelope calculation tells you that there are many thousands of G-type stars in the \textsl{Kepler}
  data that provide enough \emph{photons} to in principle permit a detection of an Earth analog.
The reason the detections are challenging is that the stars tend to vary stochastically (the Sun is unusually quiet).
At low signal-to-noise, statistical choices matter deeply;
  if we model this noise we want to do so in such a way that it is both flexible and possible to marginalize out in finite time.
The obvious choice is a Gaussian Process (GP); but with hundreds of thousands of data points,
  the standard linear algebra gets too heavy; in particular, to compare models or marginalize, one needs to take \emph{determinants},
  and these are rarely coded for speed in standard packages, even sparse ones.
We took this problem to applied mathematicians Leslie Greengard and Mike O'Neil (NYU),
  and their colleague Sivaram Ambikasaran (NYU) solved it for us: 
It turns out that standard GP kernel matrices have smoothness quasi-symmetries that can be used to enormously speed up
  factorization and determinants.
Thanks to Ambikasaran and colleagues,
  we have what we currently believe is the fastest GP code in the world, to appear very soon.
Foreman-Mackey's PhD project is to find the Earth analogs.
If we find even one, it will be a huge win for both the methods and for astrophysics.
Along the way, we have also, I think, found a new calibration for the \textsl{Kepler} data that will make
  it more precise for every investigator, not just us.

Another field that is having trouble reaching its goals is that of cosmological weak lensing.
In the standard formulation of this problem,
  galaxy ``shapes'' (some moments or ellipticities) are measured in a catalog,
  and the shapes are averaged in some way to determine the weak-lensing shear.
There is a community understanding that this is inadequate, but no agreement on a ``right'' approach.
My view is that a proper measurement will require a generative model of faint galaxy images,
  so that the entire galaxy morphology---all the pixels---are involved in the shear measurement.
Since we don't have a galaxy formation theory that works at this level,
  this project will require an extremely flexible model.
My new student Vakili is exploring some of the possibilities now;
  his approach is entirely data driven:
Since we have observed (literally) billions of faint galaxies in all that imaging,
  can't we just use those to make the model?
As you can imagine, nothing is quite that simple.

\end{document}
