\documentclass[11pt, letterpaper]{article}
\usepackage{fancyheadings, color, hyperref}

% hypertex insanity
  \definecolor{linkcolor}{rgb}{0,0,0.4}
  \hypersetup{
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=linkcolor,    % color of internal links
    citecolor=linkcolor,    % color of links to bibliography
    filecolor=linkcolor,    % color of file links
    urlcolor=linkcolor      % color of external links
  }

% other colors
  \definecolor{grey}{rgb}{0.5,0.5,0.5}
  \newcommand{\deemph}[1]{\textcolor{grey}{\footnotesize{#1}}}

% Margins and spaces
  \setlength{\oddsidemargin}{0in}
  \setlength{\topmargin}{0in}
  \setlength{\headsep}{0.20in}
  \setlength{\headheight}{0.25in}
  \setlength{\textheight}{9.00in}
%  \addtolength{\textheight}{-\headsep}
%  \addtolength{\textheight}{-\headheight}
  \addtolength{\topmargin}{-\headsep}
  \addtolength{\topmargin}{-\headheight}
  \setlength{\textwidth}{6.50in}
  \setlength{\parskip}{0.5ex}
% Headings and footing
  \renewcommand{\headrulewidth}{0pt}
  \pagestyle{fancy}
  \lhead{\deemph{David W. Hogg}}
  \chead{\deemph{Extracting Everything from Astronomical Imaging}}
  \rhead{\deemph{\thepage}}
  \cfoot{}
\newcommand{\arxiv}[1]{\href{http://arxiv.org/abs/#1}{arXiv:#1}}
\newcommand{\opcit}[1]{\href{http://arxiv.org/abs/#1}{\textit{op cit}}}

\begin{document}\sloppy\sloppypar

\noindent\textbf{1. What will be the five-year impact of my work?}
% on one or more of the natural sciences? Is there a key, fundamental
% question that you are trying to answer? How will you measure progress
% towards answering this question over the five years?
\smallskip

If I tried to boil down my projects to a single question, it would be
this:
Can we make our discoveries and measurements in astronomical imaging
at the fundamental precision dictated by the photon arrival rates?
Of course this question is uninteresting without specific contexts.
Two contexts that drive me are in the search for extra-Solar planets
(exoplanets) and in the measurement of cosmological parameters.
In both domains, the signals we care most about are tiny (in any
dimensionless sense) and the astrophysics community is \emph{not}
reaching the photon limit; not even close!

In the exoplanet domain, the context that most strongly drives data
analysis is the search for \emph{Earth analogs}.
In the search for Earth analogs, the NASA \textsl{Kepler} mission has
found many astounding Earth-sized rocky planets.
The \textsl{Kepler} mission finds these by noticing the light that a
(properly aligned) exoplanet blocks when it transits---passes in front
of its host star (from our point of view).
Some of the \textsl{Kepler}-discovered planets are conceivably habitable
(\textit{eg,} Quintana \textit{et al}, \arxiv{1404.5667})
and some of them orbit Sun-like stars
(\textit{eg,} Petigura \textit{et al}, \arxiv{1311.6806}).
None of them are \emph{both}: We have yet to see a rocky planet in the
habitable zone of a Sun-like star.
This isn't because the data aren't good enough:
There are easily enough photons to do the job.
The problems are that the instrument isn't well enough calibrated, the
stars are stochastically variable, and the data searches are
computationally expensive.

We have identified at least four different lines of research that each
could improve the precision of searches or measurements in the
\textsl{Kepler} data, and I intend to pursue all four:
The first is pixel-level (or even sub-pixel) data-driven calibration.
The individual star light curves (brightness vs time) coming out of
\textsl{Kepler} are variable at the $10^{-3}$ level.
This variability can be attributed to spacecraft (s/c) pointing,
focus, and temperature variations.
That means they are shared among stars---the stars (which are
physically independent) show covariant variability inasmuch as their
variability is induced by s/c issues.
We have found in baby-step experiments that it is best to address
these issues at the pixel level (that is, model these covariances in
the pixels before those pixel values are combined into lightcurves).
Our approach is modeling pixels with pixels
(we say a tiny bit about it in Hogg \textit{et al} \arxiv{1309.0653});
it seems to work
incredibly well in \textsl{Kepler} and might set a standard for
data-driven self-calibration of imagers into the future.
This has implications for many new projects, but notably the ESA
\textsl{Euclid} mission, which is doing sensitive imaging for
cosmology and has a limited calibration observing program budget.

The second line of research is on optimized photometric estimators.
In the above, pixels were ``combined'' into lightcurves.
How?
The \textsl{Kepler} team has a magic procedure, but we have determined
that it is not optimized.
We have found ways to use the data themselves to create a scalar
objective function that can be optimized to produce optimal (or
optimized) methods for combining pixel values into lightcurves.
This could have impact not just on \textsl{Kepler} but also the NASA
\textsl{Spitzer} mission, various ground-based photometric projects
(for example, \textsl{HATNET}), and future projects such as the NASA
\textsl{Terrestrial Planet Finder}.

The third line of research is on Gaussian-Process models for
stochastic stellar variability.
Stars vary, and the stochastic component of their variability is
effectively a noise for the detection of exoplanet transits.
A Gaussian Process effectively models the stochastic variability and
gives us variability-marginalized posterior constraints on transits.
By working with Greengard's applied mathematics group at NYU, we have
been able to make these Gaussian-Process computations far, far faster
(Ambikasaran \textit{et al} \arxiv{1403.6015})
than any other currently availble implementation.
We are using this to accelerate science with \textsl{Kepler} but also
propagating it out to other scientific domains as well.
Indeed, Gaussian Processes are used in domains as diverse as
large-scale structure and neuroscience; in many cases projects are
computation-limited.
Our work with applied math here could have huge impact across all
scientific domains (see below).

The fourth line of research is on building physical models for
telescope--camera combinations.
The key idea is that if you want to saturate the Cram\'er--Rao bound
in an estimator, that estimator ought to be a maximum-likelihood
estimator for some generative probabilistic model of the data.
In the case of \textsl{Kepler}, this looks like a model of the
detector sensitivity (pixel-by-pixel or even finer in resolution) and
a point-spread function (PSF).  
We have made baby steps along these lines
(Hogg \textit{et al}, \opcit{1403.6015})
and want to bring them to full scale.
We are also doing the same for the infrared channels of the
\textsl{Hubble Space Telescope} \textsl{WFC3} imaging instrument
(Fadely \& Hogg, in prep).
This instrument and \textsl{Kepler} have a lot in common.

In the cosmological domain, the context that drives my work is weak
lensing.
Here the idea is that inhomogeneous collapsed large-scale structure
along every line of sight leads to stochastic, percent-level
distortions of every distant galaxy.
Inasmuch as the mean shape of collections of unlensed galaxies is
round, the ``shear map'' can be measured and some statistics of the
distribution of inhomogeneous mass density can be inferred.

Right now, most methods involve 

inference in imaging projects?  Can we dramatically improve the
calibration of telescopes and cameras?  Can we develop and propagate
these ideas and standards to the community as a whole?

Making \textsl{Kepler} more sensitive.

(Obviating calibration programs for big projects.)

Taking cosmological inference down to the raw pixel level.  Weak
lensing as an example of this.

Saturating known bounds on inference; in astrophysics you can often
compute (roughly) these bounds.  Conversations with JPL about this
general point.

Evaluation: Are we improving uncertainties on known small exoplanets
around Sun-like stars?  Are we increasing the periods and decreasing
the radii of discovered exoplanets?  Are we doing better and better on
GREAT3 and GREAT4 and so on?  Are projects adopting self-calibration
programs and duplicating or generalizing our methods?

\bigskip
\noindent\textbf{2. How will I advance data-science methodologies?}
% such as statistics, machine learning, automated inference, etc., to
% achieve this goal? We are particularly interested in the ways that
% the data science methodologies that you propose to develop can be
% applied to other fields beyond the one you focus on and
% shared. Please discuss these plans. What work products do you plan
% to make open source?}
\smallskip

Developing and propagating scalable Gaussian Processes.

Building radical self-calibrations with racks and racks of regressions.

Bringing ideas of causal inference into astrophysics.

Working out ideas about probabilistic inference in enormous parameter spaces,
for weak lensing and image modeling generally.

Absolutely everything I do is out in the open; all code is released
with open-source licensing (principally MIT, sometimes GPLv2).
My research day is blogged daily at
\url{http://hoggresearch.blogspot.com/}.
I also have a blog for unpublished ideas that I would like to see
implemented.
All my grant proposals and papers in preparation and code bases in
every state of development are up on the web in open code
repositories.
Even this proposal is visible publicly at
\url{http://github.com/davidwhogg/DDD}.

\end{document}

Please submit a three-page supplement to address the two questions
below, keeping the PDF document to standard margins and 11pt type as
in the pre-application.
 
1. What do you envision as the five-year impact of your work on one or
more of the natural sciences? Is there a key, fundamental question
that you are trying to answer? How will you measure progress towards
answering this question over the five years?
 
2. How will you advance data science methodologies, such as
statistics, machine learning, automated inference, etc., to achieve
this goal? We are particularly interested in the ways that the data
science methodologies that you propose to develop can be applied to
other fields beyond the one you focus on and shared. Please discuss
these plans. What work products do you plan to make open source?

Your application materials will be used by staff and an external
review panel to select about thirty finalists; notifications will be
sent out by July 1st.  Those chosen as finalists will be invited to
give twenty minute talks at the Gordon and Betty Moore Foundation in
Palo Alto, California, on July 28th and 29th. Reasonable travel
expenses will be reimbursed. We anticipate selecting the approximately
15 awardees by late August.

Your FluidReview account at http://ddd.fluidreview.com has been
reactivated to upload your materials. You may also upload an updated
bio-sketch, but this is not required. Please send any questions to
DDDsemifinalists@moore.org.
 
We look forward to receiving your full application by May 12th at noon
Pacific Time.
