\documentclass[11pt, letterpaper]{article}
\usepackage{fancyhdr, color, hyperref}

% hypertex insanity
  \definecolor{linkcolor}{rgb}{0,0,0.4}
  \hypersetup{
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=linkcolor,    % color of internal links
    citecolor=linkcolor,    % color of links to bibliography
    filecolor=linkcolor,    % color of file links
    urlcolor=linkcolor      % color of external links
  }

% other colors
  \definecolor{grey}{rgb}{0.5,0.5,0.5}
  \newcommand{\deemph}[1]{\textcolor{grey}{\footnotesize{#1}}}

% Margins and spaces
  \setlength{\oddsidemargin}{0in}
  \setlength{\topmargin}{0in}
  \setlength{\headsep}{0.20in}
  \setlength{\headheight}{0.25in}
  \setlength{\textheight}{9.00in}
%  \addtolength{\textheight}{-\headsep}
%  \addtolength{\textheight}{-\headheight}
  \addtolength{\topmargin}{-\headsep}
  \addtolength{\topmargin}{-\headheight}
  \setlength{\textwidth}{6.50in}
  \setlength{\parskip}{0.5ex}
% Headings and footing
  \renewcommand{\headrulewidth}{0pt}
  \pagestyle{fancy}
  \lhead{\deemph{David W. Hogg}}
  \chead{\deemph{Extracting Everything from Astronomical Imaging}}
  \rhead{\deemph{\thepage}}
  \cfoot{}
\newcommand{\arxiv}[1]{\href{http://arxiv.org/abs/#1}{arXiv:#1}}
\newcommand{\opcit}[1]{\href{http://arxiv.org/abs/#1}{\textit{op cit}}}

\begin{document}\sloppy\sloppypar

\noindent\textbf{1. What will be the five-year impact of my work?}
% on one or more of the natural sciences? Is there a key, fundamental
% question that you are trying to answer? How will you measure progress
% towards answering this question over the five years?
\smallskip

If I tried to boil down my projects to a single question, it would be
this:
\emph{Can we make our discoveries and measurements in astronomical imaging
at the fundamental precision dictated by the photons?}
Two contexts for this objective are in the search for extra-Solar planets
(exoplanets) and in the measurement of cosmological parameters.
In both domains, the signals we care most about are tiny (in any
dimensionless sense) and the astrophysics community is \emph{not}
reaching the photon limit; not even close!

In the exoplanet domain, the context that most strongly drives data
analysis is the search for \emph{Earth analogs}.
The NASA \textsl{Kepler} mission has
found many Earth-sized rocky planets.
It finds these by noticing the light that a
(properly aligned) exoplanet blocks when it transits---passes in front
of its host star (from our point of view).
Some of the \textsl{Kepler}-discovered planets are conceivably habitable
(\textit{eg,} Quintana \textit{et al}, \arxiv{1404.5667})
and some of them orbit Sun-like stars
(\textit{eg,} Petigura \textit{et al}, \arxiv{1311.6806}).
None of them---so far---are \emph{both}: We have yet to see a rocky planet in the
habitable zone of a Sun-like star.
This isn't because the data aren't good enough:
There are easily enough photons to do the job.
The problems are that the instrument isn't well enough calibrated, the
stars are stochastically variable, and the data searches are
computationally expensive.

We have identified at least four different lines of research that each
could improve the precision of searches or measurements in the
\textsl{Kepler} data, and I intend to pursue all four:
The first is pixel-level (or even sub-pixel) data-driven calibration.
The individual star light curves (brightness vs time) coming out of
\textsl{Kepler} are variable at the $10^{-3}$ level.
This variability can be attributed to spacecraft (s/c) pointing,
focus, and temperature variations.
That means they are shared among stars---the stars (which are
physically independent) show covariant apparent variability inasmuch as their
apparent variability is induced by s/c issues.
We have found in baby-step experiments that it is best to address
these issues at the pixel level (that is, model these covariances in
the pixels before those pixel values are combined into lightcurves).
Our approach is modeling pixels with pixels
(we say a tiny bit about it in Hogg \textit{et al} \arxiv{1309.0653});
it seems to work
incredibly well in \textsl{Kepler} and might set a standard for
data-driven self-calibration of imagers into the future.
This has implications for many new projects, but notably the ESA
\textsl{Euclid} mission, which is doing sensitive imaging for
cosmology and has a limited calibration observing program budget.

The second line of research is on optimized photometric estimators.
In the above, pixels were ``combined'' into lightcurves.
How?
The \textsl{Kepler} team has a magic procedure, but it is not optimized.
We have found ways to use the data themselves to create a scalar
objective function that can be optimized to produce optimal
methods for combining pixel values into lightcurves.
This could have impact not just on \textsl{Kepler} but also the NASA
\textsl{Spitzer} mission, and various ground-based photometric projects
(\textsl{PanSTARRS}, for example).

The third line of research is on Gaussian-Process models for
stochastic stellar variability.
Stars vary, and the stochastic component of their variability is
effectively a noise for the detection of exoplanet transits.
A Gaussian Process effectively models the stochastic variability and
gives us variability-marginalized posterior constraints on transits.
By working with Greengard's applied mathematics group at NYU, we have
been able to make these Gaussian-Process computations far, far faster
(Ambikasaran \textit{et al} \arxiv{1403.6015})
than any other currently availble implementation.
We are using this to accelerate science with \textsl{Kepler} but also
propagating it out to other scientific domains.
Indeed, Gaussian Processes are used in domains as diverse as
large-scale structure and neuroscience; in many cases projects are
computation-limited.
Our work with applied math here could have huge impact across all
scientific domains (see below).

The fourth line of exoplanet research is on building physical models for
telescope--camera combinations.
The key idea is that if you want to saturate the Cram\'er--Rao bound
in an estimator, that estimator ought to be a maximum-likelihood
estimator for some generative probabilistic model of the data.
In the case of \textsl{Kepler}, this looks like a model of the
detector sensitivity (pixel-by-pixel or even finer in resolution) and
a point-spread function (PSF).  
We have made baby steps along these lines
(Hogg \textit{et al}, \opcit{1403.6015})
and want to bring them to full scale.
We are also doing the same for the infrared channels of the
\textsl{Hubble Space Telescope} \textsl{WFC3} imaging instrument
(Fadely \& Hogg, in prep).
This instrument and \textsl{Kepler} have a lot in common.

In the cosmological domain, the context that drives my work is weak
lensing.
Here the idea is that inhomogeneous collapsed large-scale structure
along every line of sight leads to stochastic, percent-level
distortions of every distant galaxy.
Inasmuch as the mean shape of collections of unlensed galaxies is
round, the ``shear map'' can be measured and some variances of the
distribution of inhomogeneous mass density can be inferred.
Right now, most methods involve making a point estimate of the
ellipticity of every galaxy, and making a point estimate of the shear
map by averaging those individual-galaxy point estimates.
This process involves lossy steps; it cannot be fully
information-transmitting.

The approach of my group (including now Dustin Lang at CMU) is to sample
imaging models at the pixel level, and infer the shear map as an
importance-sampled hierarchical model on top of the low-level
samplings.
The fundamental idea is that information is only fully transmitted if
we pass forward our posterior inferences about every galaxy; or that the
posterior pdf is a sufficient statistic for the pixel-level data.
For the low-level samplings---which we create with an MCMC method we
wrote (Foreman-Mackey \textit{et al}, \arxiv{1202.3665})---we have
built a generalized image-modeling tool (\textsl{The Tractor}, Lang \&
Hogg, in prep) that permits the imaging calibration, the imaging PSF,
and all the galaxy parameters to be treated as unknown, or known only noisily
\textit{a priori}.
We used this system to enter the weak-lensing data-analysis
competition GREAT3 (\url{http://www.great3challenge.info/})
with collaborators from Stanford KIPAC;
we didn't win but we only started two months out.
There are many directions in which what we are doing can be improved,
including our interim priors, our flexibility for the point-spread
function model, and our flexibility for the galaxy shape or
morphology.

In parallel to this, we have been looking at creating
extremely flexible data-driven priors for PSFs and galaxies.
These have involved learning new machine-learning methods,
most notably the GPLVM
(Lawrence, \href{http://papers.nips.cc/paper/2540-gaussian-process-latent-variable-models-for-visualisation-of-high-dimensional-data.pdf}{\textit{NIPS} 2003}).
This method permits construction of a pdf
in a very high dimensional space (like that of galaxy images)
using a smaller number of unobserved latent variables.
If we can make prior pdfs over PSFs and galaxies, we can construct
posterior pdfs for the weak-lensing shear field given the pixel data.
This effectively makes it possible to ask cosmological questions in
the space of the pixel data; that is, the question ``what are the
cosmological parameters?''\ is effectively re-cast as ``what is the
likelihood of the cosmological parameters given the imaging pixel data?''
It is not totally clear whether we are two years away from living this dream
or twenty, but the performance of the GPLVM and related methods are
making us optimistic.

How do we evaluate our progress?
The simplest questions are: Are we producing the codes we want to
produce, and are they increasing the precision of our inferences?
Are the minimum detectable planetary radii decreasing and are the
maximum detectable periods increasing with each tool release?
When we take weak lensing to the pixel level, do we get better
performance on cosmological parameters?
How do we do in the GREAT3 and GREAT4 weak-lensing challenges?
Above all, and outweighing all other metrics combined:
Can we find a true Earth analog?
Any success here would be huge, for us, for the community, and for
future NASA missions like \textsl{JWST}.

But we also want our tools and ideas \emph{adopted}.
We already (prior to publication) have several users of our
optimized photometry methods and our stochastic stellar variability
models.
Colleagues in machine learning, neural science, and cosmology have
expressed interest in our fast Gaussian Process code.
We have been working with the \textsl{Euclid} calibration team to get
self-calibration methods adopted; we think we might be able to save
the mission many hundreds of hours of observations; this corresponds to an
enormous amount of money.
Adoptions like these would be important signals of success.
Along those lines, I am headed to the NASA Jet Propulsion Lab next
fall to discuss the possible establishment of end-to-end data-analysis
precision requirements on future astrophysics missions and aerospace
subcontracts.
If this discussion goes well, the value of---and community interest
in---high-end methods might increase dramatically.

\bigskip
\noindent\textbf{2. How will I advance data-science methodologies?}
% such as statistics, machine learning, automated inference, etc., to
% achieve this goal? We are particularly interested in the ways that
% the data science methodologies that you propose to develop can be
% applied to other fields beyond the one you focus on and
% shared. Please discuss these plans. What work products do you plan
% to make open source?}
\smallskip

One of my principal interests in the work I do is both capitalizing on
and contributing to methodological domains.
I already have working relationships with the research groups of
Leslie Greengard (NYU Applied Math), Jonathan Goodman (NYU Applied
Math), Brendon Brewer (Auckland Statistics), Bernhard Sch\"olkopf (MPI
Intelligent Systems) and Rob Fergus (NYU Computer Science); I
previously worked with Sam Roweis (formerly Toronto Computer Science and Google).
These collaborations have led to some recent contributions to
the methodological literature in stats (Bovy \textit{et al},
\arxiv{0905.2979}), applied mathematics (Ambikasaran \textit{et al},
\opcit{1403.6015}), and machine learning (Lang \textit{et al},
\textit{AISTATS} 2014), and we have plans for many more.
In particular, in the areas of MCMC sampling and Gaussian Processes,
we have several methodological papers among our short-term goals.

Gaussian Processes are used across an enormous range of domains from
Earth Science to medical imaging to computer vision.
There are many projects that could be tranformed by the scalings we
have improved (what used to be $N^3$ becomes $N\,\log^2N$).
We have started meetings between applied mathematics and various
domains to get our code adopted outside astrophysics, and we plan to
publish some short demonstration papers in other fields.

Some of what we have been doing is taking straightforward ideas in
stats and inference and just taking it to a much larger scale.
For example, in our work on \textsl{The Tractor} we have realized that
in principle we can find a maximum-likelihood catalog that explains
the \emph{entire} \textsl{Sloan Digital Sky Survey} imaging data set.
This would represent a ten-billion-ish parameter maximum-likelihood
optimization.
That's not deeply innovative, but---being unprecedented in
scale---it
would be publication-worthy, for its engineering aspects at the very
least.
Similarly, if we can really perform the importance-sampling
hierarchical inference for weak lensing, it will constitute one of the
highest-dimensional MCMC inferences ever performed in the physical
sciences.
That might help us advertise our engineering and software.

Finally, and perhaps most speculatively, the self-calibration methods
we have been building are inspired by ideas from \emph{causal
  inference}, where inputs and outputs of an inference are highly
informative conditional probability relationships.
If we successfully bring ideas from causal inference into
astrophysics, it opens up a new set of opportunities for domains in
the social sciences and natural sciences to interact and exchange
ideas and tools.

On the open-source question:
Absolutely \emph{everything} I do is out in the open; all code is released
with open-source licensing (everything in this proposal will be
released MIT, some of my old software is GPLv2).
I am known in the community as being a transparency and
open-source advocate; many of my projects obtain regular downloads,
bug reports, and pull requests.
My research is blogged daily at
\url{http://hoggresearch.blogspot.com/}.
I also have a blog for unpublished ideas that I would like to see
implemented.
I tweet as \texttt{@davidwhogg}.
All my grant proposals and papers in preparation and code bases in
every state of development are up on the web in open code
repositories.
Even this proposal is visible publicly at
\url{http://github.com/davidwhogg/DDD}.

\end{document}

Please submit a three-page supplement to address the two questions
below, keeping the PDF document to standard margins and 11pt type as
in the pre-application.
 
1. What do you envision as the five-year impact of your work on one or
more of the natural sciences? Is there a key, fundamental question
that you are trying to answer? How will you measure progress towards
answering this question over the five years?
 
2. How will you advance data science methodologies, such as
statistics, machine learning, automated inference, etc., to achieve
this goal? We are particularly interested in the ways that the data
science methodologies that you propose to develop can be applied to
other fields beyond the one you focus on and shared. Please discuss
these plans. What work products do you plan to make open source?

Your application materials will be used by staff and an external
review panel to select about thirty finalists; notifications will be
sent out by July 1st.  Those chosen as finalists will be invited to
give twenty minute talks at the Gordon and Betty Moore Foundation in
Palo Alto, California, on July 28th and 29th. Reasonable travel
expenses will be reimbursed. We anticipate selecting the approximately
15 awardees by late August.

Your FluidReview account at http://ddd.fluidreview.com has been
reactivated to upload your materials. You may also upload an updated
bio-sketch, but this is not required. Please send any questions to
DDDsemifinalists@moore.org.
 
We look forward to receiving your full application by May 12th at noon
Pacific Time.
