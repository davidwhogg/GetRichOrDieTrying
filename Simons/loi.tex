\documentclass[12pt]{article}
\usepackage{fancyheadings, color, hyperref}

% hypertex insanity
  \definecolor{linkcolor}{rgb}{0,0,0.4}
  \hypersetup{
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=linkcolor,    % color of internal links
    citecolor=linkcolor,    % color of links to bibliography
    filecolor=linkcolor,    % color of file links
    urlcolor=linkcolor      % color of external links
  }
% Margins and spaces
  \setlength{\oddsidemargin}{0in}
  \setlength{\topmargin}{0in}
  \setlength{\headsep}{0.20in}
  \setlength{\headheight}{0.25in}
  \setlength{\textheight}{9.00in}
  \addtolength{\topmargin}{-\headsep}
  \addtolength{\topmargin}{-\headheight}
  \setlength{\textwidth}{6.50in}
  \setlength{\parskip}{0.5ex}
% Headings and footing
  \renewcommand{\headrulewidth}{0pt}
  \pagestyle{fancy}
  \lhead{\textsf{Goodman, Hogg \& O'Neil: \textit{Making Astrophysical Inferences Tractable}}}
  \rhead{\textsf{\thepage}}
  \cfoot{}

\newcommand{\hoggitem}{$\bullet$}

\begin{document}\sloppyy\sloppypar\thispagestyle{empty}

\noindent
\textbf{Simons Collaboration: Making Precise Astrophysical Inferences Tractable}\\
\textit{Collaboration Director:} Jonathan Goodman (NYU)\\
\textit{Core PIs:} Jonathan Goodman, David W. Hogg, Mike O'Neil (NYU)
\bigskip

We propose a Simons Collaboration working at the intersection of
Applied Mathematics and Astrophysics.
The focus of the collaboration would be on probabilistic inference
(think: Bayes), where astrophysics has benefitted from huge successes.
The thinking in the astrophysics community about the next generation of
projects, however, is currently limited by computational tractability:

\hoggitem~For many problems it is nearly impossible to write down (let
alone compute) a likelihood function.
\hoggitem~Even when there \emph{is} a likelihood function, execution
can be very slow, and it usually has to be executed enormous numbers
of times in some kind of Markov Chain Monte Carlo method for
inference.
\hoggitem~There are often enormous numbers of nuisance parameters or
even continuous functions of space and time that need to be
marginalized out.
\hoggitem~Propagtion of noise from the pixel level in detectors (where
it is well understood) to the scientific quantities of most importance
can involve exceedingly complex hierarchical structure.
\hoggitem~At the ``bottom'' of this hierarchical structure is usually
the telescope and camera (and possibly spectrograph) that took the
data in the first place; this is part of the physical process by which
the measurements were made and must be modeled along with all
astrophysical signals.
\hoggitem~Many of the signals we care about---here we focus on
extra-solar planets and cosmological large-scale structure---are
extremely small and can only be detected by principled simultaneous
analyses of large collections of data.

The Core Team of PIs already has a good record at developing ideas of
applied mathematics in astrophysics contexts that address and
ameliorate these issues in specific astrophysics settings.
The Core Team has also had good success in propagating those methods
in the astrophysics community---and a wider community of physical
scientists---through publications, pedagogical events, and
high-quality software tools.
Our Collaboration proposal would involve growing our team into
something of order eight PIs across several institutions, recruiting
in a very targeted way from the Applied Mathematics and Astrophysics
communities.

\paragraph{Our contributions}

Contemporary physics---and science in general---increasingly relies on
finding subtle but important signals in masses of noisy data.
Recent examples include the Higgs boson discovery, and the claim of
signatures of inflation in cosmic background polarization.
Both of these relied on extremely complex, approximate probabilistic
inferences, performed on very large data sets subject to enormously
complex auxilliary (non-fundamental) physical effects and non-trivial
noise.
In both cases, the signal of importance is a tiny fraction or a tiny
perturbation on the primary signals in the data set.
In its own research projects, the Core Team has been looking for two
kinds of important signals in large data sets:

One is extra-solar planet (exoplanet) signals in precise
radial-velocity and photometric measurements of stars.

Here the exoplanet signals are tiny; the planets of greatest interest
produce signals that are smaller in amplitude than typical
contributions of stochastic stellar variability (from surface
convection, helioseismology, and starspots).

...Fengji one

...emcee ...Citations!

...Fengji two

...george + HODLR ...Barclay paper

The other kind of signal we have been looking for is the weak-lensing

...Great, Schneider, etc.

An important unifying component to all of this is camera modeling
(even spectrographs are cameras).

...Tractor and K2

...Fergus and Sch\"olkopf



\section*{Our contributions}
Hogg had build several productive collaborations with computational mathematicians, including
Goodman and O'Neil.
In both cases, the work created computational methods and quality software that have enabled
new science.
The Emcee Hammer package that made possible (....David ..?). 
It is a sampler for multivariate probability distributions such as those that arise
as posteriors in Bayesian statistical analyses.
It has the property of {\em affine invariance}, which means its performance is uneffected  
by linear changes of variable.
This allows sampling from distributions that are arbitrarily ill-conditioned.
Such distributions arise whenever components of a random object have different
units (time, distance, mass, angle), and also when the data impose approximate relations 
between variables without bounding variables individually.

The Emcee Hammer package (and ???) have grown into a large well-tested software system
that has dozens of active contributors and hundreds of users.
Several developments, including parallel MCMC, were driven by users who were not
initially collaborators (Dan, is this true?).

Bayesian posterior sampling is sometimes impractical because the forward model is too 
expensive to allow good sampling even with a very good MCMC algorithm.
One example is Gaussian process models with many data points that require a determinant
of a large dense matrix ($10^4\times 10^4$?) each likelihood call.
The fast linear package ??(name)?? made this possible for the first time.
That fast determinant software was subsequently used by (?? other team) for XXX.

\section*{New research}
\subsection*{New samplers **too many words here**}
We have several ideas for much better MCMC samplers.
One, which is partly developed, is an MCMC version of the Gauss Newton method for
nonlinear least squares.
It is common to fit parameters by optimizing least squares fits of nonlinear models to data.
Sophisticated Gauss Newton Marquardt algorithms use user developed software to evaluate the 
model and the Jacobian matrix of model sensitivities.
We have an MCMC algorithm that uses this derivative information to become affine invariant.
The proposals are Gaussian centered on the Gauss Newton point.
We have discovered a robust backoff strategy related to line search without which the 
algorithm is extremely bad, but with which far out-performs the Emcee Hammer package on 
some hard exoplanet orbit fitting problems.

We are also working on a Gaussian process model of the posterior log-likelihood surface.
This uses ideas from experimental design and stochastic gradient descent in the space of
log-likelihood surfaces.
Experiments with a 2D highly non-Gaussian Rosenbrock model show that it identifies
the log-likelihood surface reasonably well in just 50 likelihood evaluations.


\subsection*{Evidence based model selection}
\subsection*{??}

\section*{The research group and funding level}
The three central faculty of this proposal are Goodman, Hogg, and O'Neil.
Current graduate student Foreman Mackey, who expects to finish this spring,
would continue as a postdoctoral fellow.
We plan for the final proposal to recruit three more tenured, tenure track, or
equivalent level members at other institutions.
Significant funding will go to an annual multi-disciplinary workshop and other
outreach activities.
We plan to hire six postdoctoral fellows and support six graduate students, in 
equal proportion in mathematics and astrophysics.
Collaborations within NYU will be facilitated by the Center for Data Science, which
this project has significant synergy with.

We estimate a final budget of approximately \$1.2M/year.

Our research and outreach activities will lead to much interdisciplinary knowledge transfer
as computational mathematicians learn the special features of problems from astrophysics
and astrophysicists learn new mathematical and computational methods.  
This interaction will be especially intense at the faculty and postdoc level.


\section*{Community involvement and outreach}
One of the distinctive features of our research is the way we interact and plan to interact
with the larger mathematical and astrophysics communities.
In addition to meetings within our group, we are planning several ways to reach out to
and involve the larger computational mathematics and astrophysics communities.
We will hold special sessions on Bayesian computation (?) at the annual ?? meetings.
We will organize minisymposia on computational problems in astrophysics at annual
SIAM meetings. 
We will organize longer one week workshops that bring together mathematicians, statisticians,
computer scientists, and astrophysicists not directly involved in the collaboration of this proposal.
We will invite members of these larger communities to contribute to and collaborate with 
software we support.









\end{document}
