% Copyright 2014 the authors.  All rights reserved.

% ## style guide:
% - nothing higher level than \paragraph{}
% - line break between sentences (that means you, O'Neil)
% - capitalize Collaboration
% - products and brands are \project{foo}

\documentclass[12pt]{article}
\usepackage{fancyheadings, color, hyperref}
\usepackage{tabto}
% hypertex insanity
  \definecolor{linkcolor}{rgb}{0,0,0.4}
  \hypersetup{
    colorlinks=true,        % false: boxed links; true: colored links
    linkcolor=linkcolor,    % color of internal links
    citecolor=linkcolor,    % color of links to bibliography
    filecolor=linkcolor,    % color of file links
    urlcolor=linkcolor      % color of external links
  }
% Margins and spaces
  \setlength{\oddsidemargin}{0in}
  \setlength{\topmargin}{0in}
  \setlength{\headsep}{0.20in}
  \setlength{\headheight}{0.25in}
  \setlength{\textheight}{9.00in}
  \addtolength{\topmargin}{-\headsep}
  \addtolength{\topmargin}{-\headheight}
  \setlength{\textwidth}{6.50in}
  \setlength{\parskip}{0.5ex}
% Headings and footing
  \renewcommand{\headrulewidth}{0pt}
  \pagestyle{fancy}
  \lhead{\textsf{Goodman, Hogg \& O'Neil: \textit{Making Astrophysical Inferences Tractable}}}
  \rhead{\textsf{\thepage}}
  \cfoot{}

  \usepackage{mathpazo}
    \usepackage{pdfpages}



\newcommand{\foreign}[1]{\textsl{#1}}
\newcommand{\etal}{\foreign{et~al.}}
\newcommand{\project}[1]{\textsl{#1}}

\newcommand{\hoggitem}{$\bullet$}
\newcommand{\bioindent}{\tabto{1.5in}}

\usepackage[margin=1in]{geometry}
\usepackage{url}

\begin{document}\sloppy\sloppypar\thispagestyle{empty}

\begin{center}
\textbf{Simons Collaboration: Making Precise Astrophysical Inferences
Tractable}\\
\textit{Collaboration Director:} Jonathan Goodman (Mathematics, NYU)\\
\textit{Core P.I.'s:} Jonathan Goodman, David W. Hogg (Physics, NYU), 
Mike O'Neil (Mathematics, NYU)\\
\textit{Collaborating faculty:} Tom Hou (Applied Mathematics, Caltech)
\end{center}
\bigskip

We propose a Simons Collaboration working at the intersection of applied
mathematics and astrophysics.
% This isn't German.  Nouns aren't capitalized.
The collaboration will focus on robust probabilistic inference 
and detection.
Probabilistic inference methods have already led to important successes in
astrophysics, including in the analysis of {\em Kepler} mission data,
but many calculations and simulations are still out of reach due to their 
apparent computational intractability.
This collaboration will bring together leaders in
modern computational and applied mathematics (fast algorithms, numerical
analysis, computational stochastics, etc.) with those at the forefront of
% I don't want to say computational statistics because we're not statisticians.
developing probabilistic and statistical models of astrophysical
phenomena with the goal of overcoming many of these computational
bottlenecks.

Most large-scale probabilistic modeling via numerical
methods faces two well-known computational bottlenecks: likelihood
evaluation and stochastic simulation.
In the case where a likelihood function can be analytically written down
(and this is not always possible), its evaluation can be numerically
intractable, often involving the application or inversion of
large-scale, {\em dense} matrices.
Even when the numerical evaluation of the likelihood function can be
made efficient, inference methods require that it still be
performed an enormous number of times from within an optimization or
Markov Chain Monte Carlo (MCMC) simulation (we have ignored any
additional marginalization of nuisance parameters).

For example, the propagation of noise from the pixel level in detectors
(where it is well understood) to the scientific quantities of most
importance can involve exceedingly complex hierarchical dependencies.
Each level of this hierarchical structure often involves the full
statistical inference procedure that we just described.
At the {\em bottom} of this hierarchical structure is usually the
telescope and camera (and possibly spectrograph) that took the data in
the first place; a precise first-principles model of the physical
process by which the measurements were made must be constructed in order
to properly interpret and quantify the astrophysical signals.
Many of the signals we will focus on detecting -- namely the existence
of extra-solar planets and cosmological large-scale structure -- are
extremely small and can only be detected via a robust simultaneous
analysis of several large collections of data.

Just as important as developing new methods, the Core Team intends to
propagate these methods within the astrophysics community -- and a wider
community of physical scientists -- through publications, pedagogical
events, and high-quality software tools. 
We have a proven history of success in this area.
Our Collaboration proposal will involve growing our team into one on the
order of six to eight P.I.'s across several institutions, selectively
recruiting from the Applied Mathematics and Astrophysics communities.

\paragraph{Our contributions}

%In particular, Goodman and Hogg's recent collaboration has yielded one
%of the most robust methods for Markov Chain Monte Carlo methods.
%Furthermore, Hogg and O'Neil's recent collaboration resulted in what is
%currently the asymptotically fastest algorithm for the computational
%manipulation of low-dimensional Gaussian processes.

The Core P.I.'s already have an excellent track record of developing
techniques to address computational roadblocks in astrophysics which
utilize recent advances in applied mathematics. We outline some of the
recent work below.

Contemporary physics -- and science in general -- increasingly relies on
finding subtle but important signals in masses of noisy data.
In its work to date, the Core Team has mainly been searching for two
types of important signals in large data sets: (i) extra-solar planet
(exoplanet) signals in precise radial-velocity and photometric
measurements of stars, and (ii) weak-lensing distortions to galaxies
induced by large-scale structure in the density (dark matter).

Exoplanet signals are tiny -- often on the order of $0.01\%$???; the
planets of greatest interest produce signals that are smaller in
amplitude than typical contributions of the background stochastic
stellar variability (from surface convection, helioseismology, and
starspots).
Thousands of planets have been found despite this, but each (even small)
improvement in modeling {\em either} the physical exoplanetary systems
or the stochastic variability of the stars has led to better
measurements and new discoveries.


We currently have several successful exoplanet projects along these
lines.
In particular, work by Goodman and Hogg has resulted in a suite of novel
MCMC samplers (Hou \etal, 2012; Foreman-Mackey \etal, 2013; Hou \etal,
2014), some of which are based on a self-tuning (affine-invariant)
ensemble proposal (Goodman \& Weare, 2010).
Some of these samplers allow for the computation of the fully
marginalized likelihood, which can inform subsequent decisions in
probabilistic inferences.
A MCMC code written by Foreman-Mackey is currently receiving a dozen
citations per month in the refereed literature across a wide range of
disciplines.
Furthermore, a recent project spearheaded by O'Neil and Hogg resulted in
asymptotically (nearly-optimal) fast, high-accuracy linear algebra
routines for calculating the inverse and determinant of the large-scale
dense covariance matrices which appear in Gaussian Process modeling
(Ambikasaran \etal, 2014).
The resulting code implementation can invert million-by-million matrices
to nearly machine precision on standard laptops and is now being used by
several researchers in varying fields, not only generating just
downloads, but requests for open-source collaboration on GitHub.
Both of these methods have made it possible to resolve some
controversies in the astronomical literature, including planet
multiplicity in a radial-velocity system (Hou \etal, 2014) and
properties of some transits around a highly variable giant star (Barclay
\etal, 2014).
%They will also make new exoplanet searches more sensitive.
We also produced the first fully probabilistic hierarchical inference of
the exoplanet population (Foreman-Mackey \etal, 2014) based on an
earlier importance-sampling method of ours (Hogg \etal, 2010).
This hierarchical inference yielded different (by a factor of seven for
the abundance of Earth-like planets) answers than previous inferences,
due in part to far more responsible use of the data (propagating noise)
and far less restrictive assumptions.

Similarly, weak-lensing distortions ($\sim 0.1\%$) are hard to find
due to the complex morphologies of galaxies.
Our biggest contribution to this problem so far is to deliver a system
that can compute the likelihood of the \emph{pixels} of the read-out
imaging from the telescope in terms of the cosmological parameters
(Schneider \etal, in prep.).
This approach relies on several of the techniques present in our exoplanet
work, namely importance sampling.
We recently entered our system in the GREAT3 weak lensing challenge, and
the details are currently being written up (Mandelbaum \etal, in prep.).
The pixel-level component of this project involves image modeling,
where we have developed an extremely general and flexible framework.
We have released an open-source code, \project{The~Tractor} (Lang \etal,
in prep.), which is now being used in several cosmological projects.

One important unifying component of our recent work is camera modeling
(including spectrographs).
In addition to \project{The~Tractor} and several ongoing first-principle
{\em physical} models, we have been investigating camera models which
are completely {\em data-driven}.
In particular, with Rob Fergus (NYU), we are building
computer-vision-based models for complex astronomical imagers (Fergus
\etal, 2014).
These models have made it possible, for the first time ever, to perform
infrared spectroscopy of several young exoplanets (Oppenheimer \etal,
2014).
Furthermore, with Bernhard Sch\"olkopf (MPI-IS), we are using ideas from
causal inference to separate intrinsic from spacecraft-induced
variability in NASA \project{Kepler} light-curves (Wang \etal, in prep.;
Foreman-Mackey \etal, in prep.).
In short, these models make predictions for stars using other stars.
These models have enormous flexibility which allow for many stars to be
used as input, as well as for the use of finely-tuned functions of those
stars in the prediction step.
Lastly, we are working on data-driven models of the \project{Kepler}
focal plane to improve photometry in the \project{K2} mission, the
post-\project{Kepler} mission in which the spacecraft is unable to point
as precisely as in the original design (Montet \etal, in prep).




\paragraph{New Directions}

We have identified many new mathematical and astrophysical avenues for
research in the Collaboration.
The details of what we finally propose will depend precisely on the
P.I.'s that we recruit (see below) and the joint-abilities that we
identify among them.
Here, for example, are several directions that we have already
identified as clearly promising.

\hoggitem~\textbf{Effective models of astronomical instruments} HOGG

\textbf{First principles camera simulation:} 
The large lenses contained in many telescopes are sometimes millions of
wavelengths in diameter, and therefore cannot be directly modeled using
standard methods in computational electromagnetics.
While geometric optics and diffraction have proven extremely useful over
the years, many simulations of speckle??? and point-spread functions are
still out of reach.
The combination of moden fast numerical algorithms and recent results in
high-frequency asymptotics should prove useful in constructing accurate
forward simulations of telescopic hardware.  

\hoggitem~\textbf{Noise models for stochastic variable stars} HOGG

\textbf{New MCMC methods:}
We have many ideas for developing next-generation MCMC samplers.
One, which is partly developed, is an MCMC version of the Gauss-Newton
method for nonlinear least squares.
This formulation extendeds MCMC using sophisticated
Gauss-Newton-Marquardt algorithms to methods which are affine-invariant
by using model derivative information.
Novel adaptive step-size control schemes improve on almost all existing
MCMC implementations.

%It is common to fit parameters by optimizing least squares fits of
%nonlinear models to data.  Sophisticated Gauss Newton Marquardt
%algorithms use user developed software to evaluate the model and the
%Jacobian matrix of model sensitivities.  We have an MCMC algorithm that
%uses this derivative information to become affine invariant.  The
%proposals are Gaussian centered on the Gauss Newton point.  We have
%discovered a robust backoff strategy related to line search without
%which the algorithm is extremely bad, but with which far out-performs
%the Emcee Hammer package on some hard exoplanet orbit fitting problems.

\textbf{Posterior likelihood modeling:}
Using ideas from experimental design and stochastic gradient
descent, we plan to model the posterior log-likelihood surface using
Gaussian Processes.
Preliminary numerical experiments show that our technique is able to
identify some highly non-Gaussian models two-dimensions in as few as
fifty likelihood evaluations.


\hoggitem~\textbf{Evidence-based model selection} GOODMAN!

\textbf{Novel analysis-based methods for dense linear algebra:}
The Core Team has already succeeded in producing algorithms for Gaussian
Processes which scale nearly-linearly in the dimension of the likelihood
function.
Further improvements to this algorithm can be made, namely optimizations
for distributed computing environments, improvements in the accuracy
which rely on the analytic formulation of the covariance kernel, and
extensions to hierarchical probabilistic structures.

\hoggitem~\textbf{Hierarchical models of exoplanet populations} HOGG

\textbf{Marginalization of the cosmological density field:} 
... requires a robust and flexible representation of the two-point
autocorrelation function.
Several methods from numerical approximation theory and applied analysis
seem to indicate that this can be made into a computationally tractable 
problem.
A flexible parameterization of correlation functions will surely have
wide-ranging applications in various other fields.




\paragraph{Collaboration personnel and budget}

The three Core P.I.'s for this proposal are Goodman, Hogg, and O'Neil.
Current graduate student Foreman-Mackey (NYU), who expects to finish
this spring, will -- if we can retain him -- continue as a postdoctoral
fellow.
We plan for the full proposal to recruit three to five more tenured,
tenure track, or equivalent level members at other institutions.  Our
current list of recruiting candidates includes: Tom Hou (Caltech), John
Johnson (Harvard), Mario Juric (UW), Phil Marshall (KIPAC), Bernhard
Sch\"olkopf (MPI-IS), ADD NAMES HERE.
We will plan to hire eight postdoctoral fellows and support eight
graduate students, in equal proportion in mathematics and astrophysics.
Collaborations within NYU will leverage the NYU Center for Data Science,
with which this project has significant relevance.
We estimate a final budget of approximately 2.1M USD per year,
based on a total of eight P.I.'s and their commitments approximate.

\paragraph{Collaboration plan and community involvement}

Our research and outreach activities will lead to a mutually beneficial
transfer of knowledge from computational mathematicians to
astrophysicists, and vice versa.
The mathematicians on our team will learn the special features of
problems from astrophysics, and astrophysicists will be exposed to new
mathematical and computational methods.
Within the Collaboration, this means quality time spent translating
problems, methods, terminology, and notation between fields.

Externally, one of the distinctive features of our research is the way
in which we interact, and plan to interact, with the larger mathematical
and astrophysics communities.
In addition to an annual Collaboration meeting in New York at the Simons
Foundation -- where we would invite not just our team but relevant
outsiders -- we expect to engage the broader community through {\em Hack
Days} and tutorials at professional meetings.
This approach has already proven to be extremely successful with regard
to the impact of our MCMC and Gaussian Process software packages.

%It is already the case that some of the impact of our MCMC and
%Gaussian Process code bases has been generated by our work operating Hack
%Days at the American Astronomical Society (AAS) annual meetings and the
%dotastronomy meetings for astronomy engineering, and also the annual
%Astro Hack Week we have just started with collaborators at UW and
%Berkeley.

We will propose to add events to the annual winter American Astronomical
Society (AAS) meetings during the Collaboration funding period.
We will do the same at the Society of Industrial and Applied Mathematics
(SIAM) meetings, and possibly the Joint Statistical Meetings (JSM).
These events will most likely take the form of
Applied-Math-meets-Astronomy Hack Days, which have previously proven to
be exceptionally successful.

Equally important, we intend for all work in the Collaboration to
proceed along a fully {\em open} model, as the work of the Core Team
already exemplifies.
We expect that a significant part of our community involvement will
proceed through the sharing of software packages, as well as projects in
progress.
In particular, all the publications from the Collaboration would be
supported by open-source code with liberal licensing (MIT or similar).


\end{document}
